{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import Model\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SageMaker Role ARN: arn:aws:iam::796932308591:role/service-role/SageMaker-ExecutionRole-20250214T145019\n"
     ]
    }
   ],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "print(f\"SageMaker Role ARN: {role}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import inference_validator, reasoning, feature_engineering, additional_feature_engineering\n",
    "import pickle, json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import boto3\n",
    "# import pickle\n",
    "# import json\n",
    "# import io\n",
    "# # Initialize S3 client\n",
    "# s3 = boto3.client('s3')\n",
    "\n",
    "# # Define S3 bucket and file path\n",
    "# bucket = 'mitrailabs-personaclassification'\n",
    "\n",
    "# # Function to load pickle files from S3\n",
    "# def load_pickle_from_s3(bucket, key):\n",
    "#     response = s3.get_object(Bucket=bucket, Key=key)\n",
    "#     return pickle.load(io.BytesIO(response['Body'].read()))\n",
    "\n",
    "# # Function to load JSON file from S3\n",
    "# def load_json_from_s3(bucket, key):\n",
    "#     response = s3.get_object(Bucket=bucket, Key=key)\n",
    "#     # response_json\n",
    "#     # io.BytesIO(file_obj['Body'].read())\n",
    "#     return json.load(response['Body']) \n",
    "\n",
    "# # Load all objects from S3\n",
    "# model_prefix = 'risk_prediction/Intermediate_states/'\n",
    "\n",
    "# nimputer = load_pickle_from_s3(bucket, f\"{model_prefix}nimputer.pkl\")\n",
    "# cimputer = load_pickle_from_s3(bucket, f\"{model_prefix}cimputer.pkl\")\n",
    "# label_encoders = load_pickle_from_s3(bucket, f\"{model_prefix}label_encoders.pkl\")\n",
    "# inference_preprocessor = load_pickle_from_s3(bucket, f\"{model_prefix}inference_preprocessor.pkl\")\n",
    "\n",
    "# before_columns = load_json_from_s3(bucket, f\"{model_prefix}before_feature.json\")\n",
    "\n",
    "\n",
    "# model_prefix = 'risk_prediction/models/'\n",
    "\n",
    "# xgb = load_pickle_from_s3(bucket, f\"{model_prefix}xgb_model.pkl\")\n",
    "\n",
    "# # ✅ Now all objects are loaded from S3 into memory\n",
    "# print(\"All files loaded successfully from S3!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3 = boto3.client('s3')\n",
    "\n",
    "# bucket = 'mitrailabs-personaclassification'\n",
    "\n",
    "\n",
    "# model_prefix = 'risk_prediction/Intermediate_states/'\n",
    "# nimputer_response = s3.get_object(\n",
    "#     Bucket=bucket,\n",
    "#     Key=f\"{model_prefix}nimputer.pkl\"\n",
    "# )\n",
    "\n",
    "# nimputer = pickle.load(nimputer_response['Body'])\n",
    "\n",
    "\n",
    "# with open('saved/nimputer.pkl', 'rb') as f:\n",
    "#     nimputer = pickle.load(f)\n",
    "\n",
    "# with open('saved/cimputer.pkl', 'rb') as f:\n",
    "#     cimputer = pickle.load(f)\n",
    "\n",
    "# with open('saved/label_encoders.pkl', 'rb') as f:\n",
    "#     label_encoders = pickle.load(f)\n",
    "\n",
    "    \n",
    "# with open('saved/inference_preprocessor.pkl', 'rb') as f:\n",
    "#     inference_preprocessor = pickle.load(f)\n",
    "\n",
    "# with open('saved/xgb_model.pkl', 'rb') as f:\n",
    "#     xgb = pickle.load(f)\n",
    "\n",
    "# with open('saved/before_feature.json', 'r') as f:\n",
    "#     before_columns = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class ManualTargetEncoder:\n",
    "    def __init__(self, smoothing=1.0):\n",
    "        \"\"\"\n",
    "        Initialize the encoder.\n",
    "        :param smoothing: Smoothing parameter to balance between category mean and global mean.\n",
    "        \"\"\"\n",
    "        self.smoothing = smoothing\n",
    "        self.encodings = {}  # Store encodings for each categorical column\n",
    "        self.global_mean = None  # Store the global mean of the target\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the encoder on the training data.\n",
    "        :param X: DataFrame containing categorical columns.\n",
    "        :param y: Target variable.\n",
    "        \"\"\"\n",
    "        self.global_mean = y.mean()\n",
    "\n",
    "        for col in X.columns:\n",
    "            # Calculate the mean target for each category\n",
    "            category_means = y.groupby(X[col]).mean()\n",
    "            # Calculate the count of each category\n",
    "            category_counts = y.groupby(X[col]).count()\n",
    "            # Apply smoothing\n",
    "            smoothed_encoding = (category_means * category_counts + self.global_mean * self.smoothing) / (\n",
    "                        category_counts + self.smoothing)\n",
    "            # Store the encodings\n",
    "            self.encodings[col] = smoothed_encoding\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Transform the categorical columns using the learned encodings.\n",
    "        :param X: DataFrame containing categorical columns.\n",
    "        :return: Transformed DataFrame.\n",
    "        \"\"\"\n",
    "        X_transformed = X.copy()\n",
    "        for col in X.columns:\n",
    "            # Replace categories with their encodings\n",
    "            X_transformed[col] = X[col].map(self.encodings[col]).fillna(self.global_mean)\n",
    "        return X_transformed\n",
    "\n",
    "    def fit_transform(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the encoder and transform the data in one step.\n",
    "        :param X: DataFrame containing categorical columns.\n",
    "        :param y: Target variable.\n",
    "        :return: Transformed DataFrame.\n",
    "        \"\"\"\n",
    "        self.fit(X, y)\n",
    "        return self.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files loaded successfully from S3!\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import joblib\n",
    "import json\n",
    "import io\n",
    "import xgboost\n",
    "\n",
    "# Initialize S3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Define S3 bucket\n",
    "bucket = 'mitrailabs-personaclassification'\n",
    "\n",
    "# Function to load joblib files from S3\n",
    "def load_joblib_from_s3(bucket, key):\n",
    "    response = s3.get_object(Bucket=bucket, Key=key)\n",
    "    return joblib.load(io.BytesIO(response['Body'].read()))\n",
    "\n",
    "# Function to load JSON file from S3 (unchanged)\n",
    "def load_json_from_s3(bucket, key):\n",
    "    response = s3.get_object(Bucket=bucket, Key=key)\n",
    "    return json.load(response['Body'])\n",
    "\n",
    "def load_xgboost_from_s3(bucket, key):\n",
    "    response = s3.get_object(Bucket=bucket, Key=key)\n",
    "    model_data = response['Body'].read()\n",
    "    \n",
    "    # Save to a temporary file\n",
    "    temp_model_path = 'saved/xgb_model.json'\n",
    "    with open(temp_model_path, 'wb') as f:\n",
    "        f.write(model_data)\n",
    "    \n",
    "    # Load the model\n",
    "    model = xgboost.Booster()\n",
    "    model.load_model(temp_model_path)\n",
    "    return model\n",
    "\n",
    "# Load all objects from S3\n",
    "model_prefix = 'risk_prediction/Intermediate_states/'\n",
    "nimputer = load_joblib_from_s3(bucket, f\"{model_prefix}nimputer.joblib\")\n",
    "cimputer = load_joblib_from_s3(bucket, f\"{model_prefix}cimputer.joblib\")\n",
    "label_encoders = load_joblib_from_s3(bucket, f\"{model_prefix}label_encoders.joblib\")\n",
    "# inference_preprocessor = load_joblib_from_s3(bucket, f\"{model_prefix}inference_preprocessor.joblib\")\n",
    "before_columns = load_json_from_s3(bucket, f\"{model_prefix}before_feature.json\")\n",
    "\n",
    "model_prefix = 'risk_prediction/models/'\n",
    "xgb = load_xgboost_from_s3(bucket, f\"{model_prefix}xgb_model.json\")\n",
    "\n",
    "# ✅ Now all objects are loaded from S3 into memory\n",
    "print(\"All files loaded successfully from S3!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: [[0.1533128  0.3737888  0.16072789 0.15892747 0.153243  ]]\n",
      "High Risk \n",
      " Borrowers in this cluster have a high default rate (0.238) and show risky borrowing behavior. They have taken moderate-sized loans (last_amount_borrowed: 0.130) and have a relatively high debt-to-income ratio (0.100). Their credit utilization (0.014) is lower than Cluster 0, but their risk scores (score_1, score_2) are slightly lower, indicating higher risk. Their frequent borrowing and moderate risk scores suggest a higher chance of future payment issues.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Example user input\n",
    "user_input = {\n",
    "    \"score_3\": 500,                     # Min: 0, Max: 990\n",
    "    \"score_4\": 80,                    # Min: 86.191572, Max: 113.978234\n",
    "    \"score_5\": 0.5,                    # Min: 0.000035, Max: 0.999973\n",
    "    \"score_6\": 100,                      # Min: 60.663039, Max: 142.192400\n",
    "    \"risk_rate\": 0,                  # Min: 0.000000, Max: 0.900000\n",
    "    \"last_amount_borrowed\": 10000000,        # Min: 0.000000, Max: 35059.600000\n",
    "    \"last_borrowed_in_months\": 31,      # Min: 0.000000, Max: 60.000000\n",
    "    \"credit_limit\": 100000,                 # Min: 0.000000, Max: 448269.000000\n",
    "    \"income\": 1200000,                   # Min: 4821.180000, Max: 5000028.000000\n",
    "    \"ok_since\": 53,                     # Min: 0.000000, Max: 141.000000\n",
    "    \"n_accounts\": 1,                 # Min: 0.000000, Max: 49.000000\n",
    "    \"n_issues\": 10,                    # Min: 0.000000, Max: 49.000000\n",
    "    \"external_data_provider_credit_checks_last_year\": 1, # Min: 0.000000, Max: 1.000000\n",
    "    \"external_data_provider_email_seen_before\": 50,        # Min: 0.000000, Max: 59.000000\n",
    "    \"reported_income\": 120000,          # Min: 403.000000, Max: 6355500000000000.000000\n",
    "    \"application_time_in_funnel\": 300,  # Min: 0.000000, Max: 500.000000\n",
    "    \"external_data_provider_credit_checks_last_month\": 0.0, # Min: 0.000000, Max: 3.000000\n",
    "    \"external_data_provider_fraud_score\": 0, # Min: 0.000000, Max: 1000.000000\n",
    "    \"shipping_state\": np.nan,             # No min/max provided (categorical)\n",
    "    \"facebook_profile\": False,          # No min/max provided (boolean)\n",
    "    \"state\": \"BR-MS\",                      # No min/max provided (categorical)\n",
    "    \"score_1\": \"1Rk8w4Ucd5yR3KcqZzLdow==\", # No min/max provided (encoded string)\n",
    "    \"real_state\": np.nan                  # No min/max provided (nullnp.nan\n",
    "}\n",
    "\n",
    "user_input = inference_validator(user_input)\n",
    "user_df = pd.DataFrame([user_input])\n",
    "# Ensure the user input has the same columns as the training data\n",
    "user_df = user_df.reindex(columns=before_columns, fill_value=np.nan)\n",
    "\n",
    "# Assuming you know which features each imputer was trained on\n",
    "c_features = [f for f in cimputer.feature_names_in_ if f in user_df.columns]\n",
    "n_features = [f for f in nimputer.feature_names_in_ if f in user_df.columns]\n",
    "# print(c_features)\n",
    "# print(n_features)\n",
    "\n",
    "# Apply only if features exist\n",
    "if c_features:\n",
    "    user_df[c_features] = cimputer.transform(user_df[c_features])\n",
    "if n_features:\n",
    "    user_df[n_features] = nimputer.transform(user_df[n_features])\n",
    "\n",
    "user_df = user_df.drop(columns=[\"target_default\"])\n",
    "# print(\"target_default\" in user_df.columns)\n",
    "# Apply the same label encoding to the user input\n",
    "user_df[[i for i in c_features if i != \"target_default\"]] = label_encoders.transform(user_df[[i for i in c_features if i != \"target_default\"]])\n",
    "\n",
    "yo_features = [i for i in c_features + n_features if i != \"target_default\"]\n",
    "# user_df = user_df.reindex(columns=yo_features)\n",
    "# user_df = user_df.drop(columns=[\"target_default\"])\n",
    "\n",
    "user_df = feature_engineering(user_df)\n",
    "user_df = additional_feature_engineering(user_df)\n",
    "# print(user_df)\n",
    "# user_processed = inference_preprocessor.transform(user_df)\n",
    "# user_df['fraud_score_bin'] = user_df['fraud_score_bin'].astype(int)\n",
    "user_df = xgboost.DMatrix(user_df)\n",
    "# Make prediction\n",
    "prediction = xgb.predict(user_df)\n",
    "# prediction_proba = xgb.predict_proba(user_df)\n",
    "\n",
    "print(\"Prediction:\", prediction)\n",
    "print(reasoning[str(np.argmax((prediction[0])))])\n",
    "# print(\"Prediction Probability:\", prediction_proba)\n",
    "# print(user_processed)\n",
    "# 6 → 3 → 7 → 1 → 2 → 0 → 5 → 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.2\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
