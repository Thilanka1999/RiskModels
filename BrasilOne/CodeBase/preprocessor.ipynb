{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV, cross_val_score\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report, roc_curve, roc_auc_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# set the aesthetic style of the plots\n",
    "sns.set_style()\n",
    "\n",
    "# filter warning messages\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_credit = pd.read_csv('../acquisition_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data frame shape\n",
    "print('Number of rows: ', df_credit.shape[0])\n",
    "print('Number of columns: ', df_credit.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of columns for each data type\n",
    "dtype_counts = df_credit.dtypes.value_counts()\n",
    "\n",
    "# Print the results\n",
    "print(\"Number of columns by data type:\")\n",
    "print(dtype_counts)\n",
    "\n",
    "float_features = df_credit.select_dtypes(include=['float']).columns\n",
    "object_features = df_credit.select_dtypes(include=['object']).columns\n",
    "int_features = df_credit.select_dtypes(include=['int']).columns\n",
    "bool_features = df_credit.select_dtypes(include=['bool']).columns\n",
    "\n",
    "# Print the feature names for each category\n",
    "print(\"Float features:\", list(float_features))\n",
    "print(\"Object features:\", list(object_features))\n",
    "print(\"Integer features:\", list(int_features))\n",
    "print(\"Bool features:\", list(bool_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_credit.dropna(subset=['target_default'], inplace=True)\n",
    "df_credit.drop('target_fraud', axis=1, inplace=True)\n",
    "df_credit.drop(labels=['channel', 'external_data_provider_credit_checks_last_2_year'], axis=1, inplace=True)\n",
    "df_credit.drop(labels=['email', 'reason', 'zip', 'job_name', 'external_data_provider_first_name', 'lat_lon',\n",
    "                       'shipping_zip_code', 'user_agent', 'profile_tags', 'marketing_channel',\n",
    "                       'profile_phone_number', 'application_time_applied', 'ids'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_credit.drop('facebook_profile', axis=1, inplace=True)\n",
    "# df_credit.drop('external_data_provider_credit_checks_last_year', axis=1, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_credit.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count of values = -999 in \"external_data_provider_email_seen_before\"\n",
    "df_credit.loc[df_credit['external_data_provider_email_seen_before'] == -999, 'external_data_provider_email_seen_before'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace \"inf\" values with \"nan\"\n",
    "df_credit['reported_income'] = df_credit['reported_income'].replace(np.inf, np.nan)\n",
    "\n",
    "# replace \"-999\" values with \"nan\"\n",
    "df_credit.loc[df_credit['external_data_provider_email_seen_before'] == -999, 'external_data_provider_email_seen_before'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_credit = df_credit[df_credit['income'] <= 700000]\n",
    "df_credit = df_credit[df_credit['reported_income'] <= 210000]\n",
    "df_credit.drop(labels=[\"n_bankruptcies\", \"n_defaulted_loans\"], axis=1, inplace=True)\n",
    "df_credit.drop(labels=[\"score_2\"], axis=1, inplace=True) #highly corelated with score 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('saved/before_feature.json', 'w') as f:\n",
    "    json.dump(df_credit.columns.tolist(), f)\n",
    "# with open('saved/before_feature.json', 'r') as f:\n",
    "#     loaded_list = json.dump(df_credit.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_credit_num = df_credit.select_dtypes(exclude='object').columns\n",
    "df_credit_cat = df_credit.select_dtypes(include='object').columns\n",
    "\n",
    "# fill missing values for \"last_amount_borrowed\", \"last_borrowed_in_months\" and \"n_issues\"\n",
    "df_credit['last_amount_borrowed'].fillna(value=0, inplace=True)\n",
    "df_credit['last_borrowed_in_months'].fillna(value=0, inplace=True)\n",
    "df_credit['n_issues'].fillna(value=0, inplace=True)\n",
    "\n",
    "# fill missing values for numerical variables\n",
    "nimputer = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "nimputer = nimputer.fit(df_credit.loc[:, df_credit_num])\n",
    "df_credit.loc[:, df_credit_num] = nimputer.transform(df_credit.loc[:, df_credit_num])\n",
    "\n",
    "\n",
    "# fill missing values for categorical variables\n",
    "cimputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "cimputer = cimputer.fit(df_credit.loc[:, df_credit_cat])\n",
    "df_credit.loc[:, df_credit_cat] = cimputer.transform(df_credit.loc[:, df_credit_cat])\n",
    "\n",
    "import pickle   \n",
    "\n",
    "with open('saved/nimputer.pkl', 'wb') as f:\n",
    "    pickle.dump(nimputer, f)\n",
    "with open('saved/cimputer.pkl', 'wb') as f:\n",
    "    pickle.dump(cimputer, f)\n",
    "\n",
    "\n",
    "\n",
    "print(df_credit.columns)\n",
    "df_credit.to_csv('saved/preprocessed_bank_data.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_credit.nunique().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_credit.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extra stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X= df_credit.drop(columns=[\"target_default\"])\n",
    "y = df_credit['target_default']\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "# Handle categorical features using Label Encoding\n",
    "label_encoders = {}\n",
    "for column in df_credit.columns:\n",
    "    if pd.api.types.is_categorical_dtype(df_credit[column]) or pd.api.types.is_object_dtype(df_credit[column]):\n",
    "        label_encoders[column] = LabelEncoder()\n",
    "        df_credit[column] = label_encoders[column].fit_transform(df_credit[column])\n",
    "\n",
    "X = df_credit.drop(columns=[\"target_default\"])\n",
    "y = df_credit['target_default']\n",
    "# Calculate F-scores and p-values\n",
    "f_scores, p_values = f_classif(X, y)\n",
    "\n",
    "# Create a DataFrame to display feature importance\n",
    "yo = pd.DataFrame({'Feature': X.columns, 'F-Score': f_scores, 'P-Value': p_values})\n",
    "\n",
    "# Sort by F-Score (higher F-Score means more important)\n",
    "feature_importance = yo.sort_values(by='F-Score', ascending=False)\n",
    "\n",
    "# Print the feature importance\n",
    "print(feature_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "X, y = SMOTE().fit_resample(df_credit.drop(columns=['target_default']), df_credit['target_default'])\n",
    "\n",
    "print(len(X))\n",
    "f_scores, p_values = f_classif(X, y)\n",
    "\n",
    "# Create a DataFrame to display feature importance\n",
    "yo = pd.DataFrame({'Feature': X.columns, 'F-Score': f_scores, 'P-Value': p_values})\n",
    "\n",
    "# Sort by F-Score (higher F-Score means more important)\n",
    "feature_importance = yo.sort_values(by='F-Score', ascending=False)\n",
    "\n",
    "# Print the feature importance\n",
    "print(feature_importance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Compute correlation matrix\n",
    "# correlation_matrix = X.corr()\n",
    "\n",
    "# # Set figure size\n",
    "# plt.figure(figsize=(12, 8))\n",
    "\n",
    "# # Create heatmap with bigger annotations\n",
    "# sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", linewidths=0.5, annot_kws={\"size\": 10})\n",
    "\n",
    "# # Show plot\n",
    "# plt.title(\"Feature Correlation Heatmap\", fontsize=14)\n",
    "# plt.xticks(rotation=45, ha='right', fontsize=10)\n",
    "# plt.yticks(fontsize=10)\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Replace 'target' with the actual target column\n",
    "correlations = df_credit.corr()['target_default'].abs().sort_values(ascending=False)\n",
    "print(\"Feature Correlations with Target:\\n\", correlations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "df = df_credit\n",
    "# Replace with actual column names\n",
    "facebook_col = \"facebook_profile\"  # The feature to filter on\n",
    "target_col = \"target_default\"  # The target variable\n",
    "\n",
    "# Count target values when facebook_profile is True\n",
    "true_set = df[df[facebook_col] == 1][target_col].value_counts()\n",
    "\n",
    "# Count target values when facebook_profile is False\n",
    "false_set = df[df[facebook_col] == 0][target_col].value_counts()\n",
    "\n",
    "# Print results\n",
    "print(f\"Target counts when {facebook_col} is True:\")\n",
    "print(true_set)\n",
    "print(\"\\n\" + \"-\"*40 + \"\\n\")\n",
    "print(f\"Target counts when {facebook_col} is False:\")\n",
    "print(false_set)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
