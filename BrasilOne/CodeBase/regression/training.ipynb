{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV, cross_val_score\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report, roc_curve, roc_auc_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# set the aesthetic style of the plots\n",
    "sns.set_style()\n",
    "\n",
    "# filter warning messages\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# Assuming feature_importance is your DataFrame\n",
    "# ... (your code to create and sort feature_importance) ...\n",
    "\n",
    "# Set options to display all rows and columns\n",
    "# pd.set_option('display.max_rows', None)  # None means unlimited rows\n",
    "# pd.set_option('display.max_columns', None) # None means unlimited columns\n",
    "# pd.set_option('display.width', None)      # None means auto-detect width\n",
    "# pd.set_option('display.max_colwidth', None) # None means unlimited column width\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_processed = pd.read_csv('saved/feature_engineered_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_processed.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_processed = X_processed.dropna(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "# X_processed = X_processed.drop(columns=['AgeBin', 'CreditScoreBin', 'EmploymentStatus', 'MaritalStatus', 'HomeOwnershipStatus', 'EducationLevel', 'LoanPurpose'])\n",
    "\n",
    "X = X_processed.drop(columns=['RiskScore'])\n",
    "y = X_processed['RiskScore']\n",
    "# print(X.columns)\n",
    "\n",
    "f_scores, p_values = f_classif(X, y)\n",
    "\n",
    "# Create a DataFrame to display results\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'F-Score': f_scores,\n",
    "    'P-Value': p_values\n",
    "})\n",
    "\n",
    "# Sort by F-Score (higher F-Score means more important)\n",
    "feature_importance = feature_importance.sort_values(by='F-Score', ascending=False)\n",
    "\n",
    "print(feature_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, random_state=42, test_size=0.1)\n",
    "print(len(X_train), len(X_test))\n",
    "print(\"NaN in X_train:\", X_train.isna().sum().sum())\n",
    "print(\"NaN in y_train:\", y_train.isna().sum())\n",
    "print(\"Infinite values in X:\", np.isinf(y_train.values).sum())\n",
    "\n",
    "\n",
    "print(X_train.head(5))\n",
    "import json\n",
    "\n",
    "with open('saved/df_train_encoded.json', 'w') as f:\n",
    "    json.dump(X_train.columns.tolist(), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_rus, y_train_rus = (X_train, y_train)\n",
    "\n",
    "f_scores, p_values = f_classif(X_train_rus, y_train_rus)\n",
    "\n",
    "# Create a DataFrame to display feature importance\n",
    "yo = pd.DataFrame({'Feature': X_train_rus.columns, 'F-Score': f_scores, 'P-Value': p_values})\n",
    "\n",
    "# Sort by F-Score (higher F-Score means more important)\n",
    "feature_importance = yo.sort_values(by='F-Score', ascending=False)\n",
    "\n",
    "# Print the feature importance\n",
    "print(feature_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=len(X_train_rus.columns))\n",
    "pca.fit(X_train_rus)\n",
    "\n",
    "# Get explained variance ratio\n",
    "explained_variance = pca.explained_variance_ratio_ * 100\n",
    "\n",
    "# Create DataFrame\n",
    "variance_df = pd.DataFrame({'Feature': X_train_rus.columns, 'Explained Variance (%)': explained_variance})\n",
    "variance_df = variance_df.sort_values(by='Explained Variance (%)', ascending=False)\n",
    "\n",
    "# print(variance_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_rus.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor, BaggingRegressor\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "from catboost import CatBoostRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Define the models\n",
    "# models = {\n",
    "#     \"CatBoost\": CatBoostRegressor(random_state=42, verbose=0),\n",
    "#     \"LightGBM\": LGBMRegressor(random_state=42),\n",
    "#     \"Random Forest\": RandomForestRegressor(random_state=42),\n",
    "#     \"AdaBoost\": AdaBoostRegressor(random_state=42),\n",
    "#     \"Gradient Boosting\": GradientBoostingRegressor(random_state=42),\n",
    "#     \"Bagging\": BaggingRegressor(random_state=42),\n",
    "#     \"XGBoost\": XGBRegressor(random_state=42)\n",
    "# }\n",
    "models = {\n",
    "    \"CatBoost\": CatBoostRegressor(\n",
    "        iterations=1000,\n",
    "        learning_rate=0.1,\n",
    "        depth=8,\n",
    "        l2_leaf_reg=3,\n",
    "        random_state=42,\n",
    "        verbose=0\n",
    "    ),\n",
    "    \"LightGBM\": LGBMRegressor(\n",
    "        n_estimators=200,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=9,\n",
    "        num_leaves=31,\n",
    "        min_child_samples=20,\n",
    "        random_state=42\n",
    "    ),\n",
    "    \"LinearRegressor\": LinearRegression(),\n",
    "    # \"Random Forest\": RandomForestRegressor(\n",
    "    #     n_estimators=200,\n",
    "    #     max_depth=9,\n",
    "    #     min_samples_split=5,\n",
    "    #     min_samples_leaf=2,\n",
    "    #     random_state=42\n",
    "    # ),\n",
    "    # \"AdaBoost\": AdaBoostRegressor(\n",
    "    #     n_estimators=200,\n",
    "    #     learning_rate=0.1,\n",
    "    #     random_state=42\n",
    "    # ),\n",
    "    # \"Gradient Boosting\": GradientBoostingRegressor(\n",
    "    #     n_estimators=200,\n",
    "    #     learning_rate=0.1,\n",
    "    #     max_depth=5,\n",
    "    #     min_samples_split=5,\n",
    "    #     min_samples_leaf=2,\n",
    "    #     random_state=42\n",
    "    # ),\n",
    "    # \"Bagging\": BaggingRegressor(\n",
    "    #     n_estimators=200,\n",
    "    #     max_samples=0.8,\n",
    "    #     max_features=0.8,\n",
    "    #     random_state=42\n",
    "    # ),\n",
    "    \"XGBoost\": XGBRegressor(\n",
    "        n_estimators=200,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=9,\n",
    "        gamma=1,\n",
    "        min_child_weight=3,\n",
    "        random_state=42\n",
    "    )\n",
    "}\n",
    "# Dictionary to store the results\n",
    "results = {}\n",
    "\n",
    "# Train and evaluate each model\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    model.fit(X_train_rus, y_train_rus)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    # Store results\n",
    "    results[name] = {\n",
    "        \"MAE\": mae,\n",
    "        \"MSE\": mse,\n",
    "        \"RMSE\": rmse,\n",
    "        \"R²\": r2\n",
    "    }\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"{name} Metrics:\")\n",
    "    print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "    print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "    print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "    print(f\"R² Score: {r2:.4f}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Compare model performances\n",
    "print(\"Model Comparison:\")\n",
    "for name, metrics in results.items():\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  MAE: {metrics['MAE']:.4f}\")\n",
    "    print(f\"  MSE: {metrics['MSE']:.4f}\")\n",
    "    print(f\"  RMSE: {metrics['RMSE']:.4f}\")\n",
    "    print(f\"  R²: {metrics['R²']:.4f}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = CatBoostRegressor(\n",
    "        iterations=2000,\n",
    "        learning_rate=0.1,\n",
    "        depth=6,\n",
    "        l2_leaf_reg=3,\n",
    "        random_state=42,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "cat.fit(X_train_rus, y_train_rus)\n",
    "\n",
    "cat.save_model(f\"saved/cat_model.cbm\", format=\"cbm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import catboost as cb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assume you have a trained CatBoost model\n",
    "feature_importances = cat.get_feature_importance()\n",
    "\n",
    "# If you have feature names\n",
    "feature_names = cat.feature_names_\n",
    "\n",
    "# Sort and plot feature importance\n",
    "sorted_idx = np.argsort(feature_importances)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(np.array(feature_names)[sorted_idx], np.array(feature_importances)[sorted_idx])\n",
    "plt.xlabel(\"Feature Importance\")\n",
    "plt.title(\"CatBoost Feature Importance\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final XGBoost model\n",
    "xgb = XGBRegressor(\n",
    "    max_depth=9, \n",
    "    learning_rate=0.1,  \n",
    "    n_estimators=200, \n",
    "    gamma=1,  \n",
    "    min_child_weight=3,\n",
    "    random_state=42  # Ensures reproducibility\n",
    ")\n",
    "xgb.fit(X_train_rus, y_train_rus)\n",
    "# prediction\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Predictions\n",
    "y_pred_xgb = xgb.predict(X_test)\n",
    "\n",
    "# Calculate Metrics\n",
    "mae = mean_absolute_error(y_test, y_pred_xgb)\n",
    "mse = mean_squared_error(y_test, y_pred_xgb)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred_xgb)\n",
    "\n",
    "# Print Metrics\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "print(f\"R² Score: {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "xgboost.plot_importance(xgb, importance_type='gain', ax=ax) # or 'weight', 'cover'\n",
    "plt.show()\n",
    "\n",
    "print(xgb.feature_importances_)\n",
    "\n",
    "\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have your data (X, y) and trained model (model)\n",
    "# model = xgb.XGBRegressor().fit(X, y) or model = xgb.XGBClassifier().fit(X,y)\n",
    "\n",
    "# explainer = shap.TreeExplainer(xgb)\n",
    "# shap_values = explainer.shap_values(X)\n",
    "\n",
    "# # Summary plot (global importance)\n",
    "# shap.summary_plot(shap_values, X)\n",
    "\n",
    "# # Force plot (local explanation for a single prediction)\n",
    "# shap.initjs()\n",
    "# shap.force_plot(explainer.expected_value, shap_values[0,:], X.iloc[0,:]) #first row of X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model_path = 'saved/xgb_model.json'  # Use .json for XGBoost's native format\n",
    "xgb.save_model(xgb_model_path)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
