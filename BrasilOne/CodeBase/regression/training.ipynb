{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV, cross_val_score\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report, roc_curve, roc_auc_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# set the aesthetic style of the plots\n",
    "sns.set_style()\n",
    "\n",
    "# filter warning messages\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# Assuming feature_importance is your DataFrame\n",
    "# ... (your code to create and sort feature_importance) ...\n",
    "\n",
    "# Set options to display all rows and columns\n",
    "pd.set_option('display.max_rows', None)  # None means unlimited rows\n",
    "pd.set_option('display.max_columns', None) # None means unlimited columns\n",
    "pd.set_option('display.width', None)      # None means auto-detect width\n",
    "pd.set_option('display.max_colwidth', None) # None means unlimited column width\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_processed = pd.read_csv('saved/feature_engineered_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_processed.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "# X_processed = X_processed.drop(columns=['AgeBin', 'CreditScoreBin', 'EmploymentStatus', 'MaritalStatus', 'HomeOwnershipStatus', 'EducationLevel', 'LoanPurpose'])\n",
    "\n",
    "X = X_processed.drop(columns=['RiskScore'])\n",
    "y = X_processed['RiskScore']\n",
    "# print(X.columns)\n",
    "\n",
    "f_scores, p_values = f_classif(X, y)\n",
    "\n",
    "# Create a DataFrame to display results\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'F-Score': f_scores,\n",
    "    'P-Value': p_values\n",
    "})\n",
    "\n",
    "# Sort by F-Score (higher F-Score means more important)\n",
    "feature_importance = feature_importance.sort_values(by='F-Score', ascending=False)\n",
    "\n",
    "print(feature_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, test_size=0.1)\n",
    "print(len(X_train), len(X_test))\n",
    "print(\"NaN in X_train:\", X_train.isna().sum().sum())\n",
    "print(\"NaN in y_train:\", y_train.isna().sum())\n",
    "print(\"Infinite values in X:\", np.isinf(y_train.values).sum())\n",
    "\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "with open('saved/df_train_encoded.json', 'w') as f:\n",
    "    json.dump(X_train.columns.tolist(), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_rus, y_train_rus = (X_train, y_train)\n",
    "\n",
    "f_scores, p_values = f_classif(X_train_rus, y_train_rus)\n",
    "\n",
    "# Create a DataFrame to display feature importance\n",
    "yo = pd.DataFrame({'Feature': X_train_rus.columns, 'F-Score': f_scores, 'P-Value': p_values})\n",
    "\n",
    "# Sort by F-Score (higher F-Score means more important)\n",
    "feature_importance = yo.sort_values(by='F-Score', ascending=False)\n",
    "\n",
    "# Print the feature importance\n",
    "print(feature_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=len(X_train_rus.columns))\n",
    "pca.fit(X_train_rus)\n",
    "\n",
    "# Get explained variance ratio\n",
    "explained_variance = pca.explained_variance_ratio_ * 100\n",
    "\n",
    "# Create DataFrame\n",
    "variance_df = pd.DataFrame({'Feature': X_train_rus.columns, 'Explained Variance (%)': explained_variance})\n",
    "variance_df = variance_df.sort_values(by='Explained Variance (%)', ascending=False)\n",
    "\n",
    "# print(variance_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_rus.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final XGBoost model\n",
    "xgb = XGBRegressor(\n",
    "    max_depth=9, \n",
    "    learning_rate=0.1,  \n",
    "    n_estimators=200, \n",
    "    gamma=1,  \n",
    "    min_child_weight=3,\n",
    "    random_state=42  # Ensures reproducibility\n",
    ")\n",
    "xgb.fit(X_train_rus, y_train_rus)\n",
    "# prediction\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Predictions\n",
    "y_pred_xgb = xgb.predict(X_test)\n",
    "\n",
    "# Calculate Metrics\n",
    "mae = mean_absolute_error(y_test, y_pred_xgb)\n",
    "mse = mean_squared_error(y_test, y_pred_xgb)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred_xgb)\n",
    "\n",
    "# Print Metrics\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "print(f\"RÂ² Score: {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "xgboost.plot_importance(xgb, importance_type='gain', ax=ax) # or 'weight', 'cover'\n",
    "plt.show()\n",
    "\n",
    "print(xgb.feature_importances_)\n",
    "\n",
    "\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have your data (X, y) and trained model (model)\n",
    "# model = xgb.XGBRegressor().fit(X, y) or model = xgb.XGBClassifier().fit(X,y)\n",
    "\n",
    "# explainer = shap.TreeExplainer(xgb)\n",
    "# shap_values = explainer.shap_values(X)\n",
    "\n",
    "# # Summary plot (global importance)\n",
    "# shap.summary_plot(shap_values, X)\n",
    "\n",
    "# # Force plot (local explanation for a single prediction)\n",
    "# shap.initjs()\n",
    "# shap.force_plot(explainer.expected_value, shap_values[0,:], X.iloc[0,:]) #first row of X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save label encoders to disk\n",
    "with open('saved/xgb_model.pkl', 'wb') as f:\n",
    "    pickle.dump(xgb, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
