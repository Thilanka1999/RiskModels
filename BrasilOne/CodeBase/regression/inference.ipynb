{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import inference_validator, feature_engineering\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', None)  # None means unlimited rows\n",
    "pd.set_option('display.max_columns', None) # None means unlimited columns\n",
    "pd.set_option('display.width', None)      # None means auto-detect width\n",
    "pd.set_option('display.max_colwidth', None) # None means unlimited column width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groq_api_key = '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'\n",
    "import groq\n",
    "\n",
    "# Initialize Groq client\n",
    "client = groq.Client(api_key=\"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\")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def get_risk_score_reasoning(user_df, prediction):\n",
    "    \"\"\"\n",
    "    Generates a reasoning for the risk score using an LLM.\n",
    "\n",
    "    Args:\n",
    "        user_df (pd.DataFrame): User data after feature engineering.\n",
    "        prediction (np.ndarray): The predicted risk score (NumPy array).\n",
    "\n",
    "    Returns:\n",
    "        str: The reasoning for the risk score.\n",
    "    \"\"\"\n",
    "    risk_score = prediction[0]  # Extract the first element (scalar)\n",
    "    user_data_str = user_df.to_string()\n",
    "\n",
    "    # Prepare the prompt for the LLM\n",
    "    prompt = (\n",
    "        f\"You are a financial risk analyst. A user has applied for a loan, and the machine learning model has assigned them a risk score of {risk_score:.2f}.\\n\\n\"\n",
    "        f\"Below are the user's details after feature engineering:\\n\"\n",
    "        f\"{user_data_str}\\n\\n\"\n",
    "        f\"### Instructions:\\n\"\n",
    "        f\"- Provide a detailed explanation of why this person received the given risk score.\\n\"\n",
    "        f\"- Highlight key features that contributed to the risk score (e.g., high debt-to-income ratio, low credit score, late payments, etc.).\\n\"\n",
    "        f\"- Explain how these features impact the user's financial risk.\\n\"\n",
    "        f\"- Mention any factors that could improve or worsen their risk score.\\n\"\n",
    "        f\"- Provide actionable recommendations for the user to reduce their risk score if applicable.\\n\\n\"\n",
    "        f\"- Remember risk score between 27 and 84. \\n\\n\"\n",
    "        f\"### Output Format:\\n\"\n",
    "        f\"- Output ONLY a plain text explanation without any additional formatting or labels.\\n\"\n",
    "    )\n",
    "\n",
    "    # ... (rest of your LLM interaction code) ...\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"llama-3.2-3b-preview\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a financial risk analyst.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        max_tokens=250,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import joblib\n",
    "import os\n",
    "import xgboost\n",
    "import json\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    \"\"\"\n",
    "    Load model artifacts from the model_dir for SageMaker model serving\n",
    "    \n",
    "    Args:\n",
    "        model_dir (str): Directory where model artifacts are stored\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing all loaded model artifacts\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Loading model from: {model_dir}\")\n",
    "        \n",
    "        # Load all artifacts from local model_dir\n",
    "        artifacts = {}\n",
    "        \n",
    "        # Load numerical and categorical imputers (using joblib)\n",
    "        artifacts['nimputer'] = joblib.load(os.path.join(model_dir, 'nimputer.joblib'))\n",
    "        logger.info(\"Loaded nimputer\")\n",
    "        \n",
    "        artifacts['cimputer'] = joblib.load(os.path.join(model_dir, 'cimputer.joblib'))\n",
    "        logger.info(\"Loaded cimputer\")\n",
    "        \n",
    "        # Load label encoders (still using pickle)\n",
    "        artifacts['ordinalencoder'] = joblib.load(os.path.join(model_dir, 'ordinalencoder.joblib'))\n",
    "        logger.info(\"Loaded ordinalencoder\")\n",
    "\n",
    "        artifacts['targetencoder'] = joblib.load(os.path.join(model_dir, 'targetencoder.joblib'))\n",
    "        logger.info(\"Loaded targetencoder\")\n",
    "        \n",
    "        # Load feature list (JSON remains unchanged)\n",
    "        with open(os.path.join(model_dir, 'before_feature.json'), 'r') as f:\n",
    "            artifacts['before_columns'] = json.load(f)\n",
    "        logger.info(\"Loaded before_columns\")\n",
    "\n",
    "        # Load the model\n",
    "        model_path = os.path.join(model_dir, 'xgb_model.json')\n",
    "        model = xgboost.XGBRegressor()\n",
    "        model.load_model(model_path)\n",
    "        artifacts['xgb'] = model\n",
    "        logger.info(\"Loaded xgb model\")\n",
    "        \n",
    "        # Add the reasoning dictionary \n",
    "        logger.info(\"All model artifacts loaded successfully\")\n",
    "        return artifacts\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading model: {str(e)}\")\n",
    "        raise\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn(request_body, request_content_type):\n",
    "    \"\"\"\n",
    "    Deserialize and prepare the prediction input\n",
    "    \n",
    "    Args:\n",
    "        request_body: The request body\n",
    "        request_content_type: The request content type\n",
    "        \n",
    "    Returns:\n",
    "        dict: Input data in dictionary format\n",
    "    \"\"\"\n",
    "    logger.info(f\"Received request with content type: {request_content_type}\")\n",
    "    \n",
    "    if request_content_type == 'application/json':\n",
    "        try:\n",
    "            input_data = json.loads(request_body)\n",
    "            logger.info(f\"Parsed input data: {str(input_data)[:100]}...\")\n",
    "            return input_data\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error parsing JSON input: {str(e)}\")\n",
    "            raise ValueError(f\"Error parsing JSON input: {str(e)}\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported content type: {request_content_type}. Only application/json is supported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from encoders import CustomTargetEncoder, CustomOrdinalEncoder\n",
    "\n",
    "def predict_fn(input_data, model_artifacts):\n",
    "    \"\"\"\n",
    "    Apply model to the input data\n",
    "    \n",
    "    Args:\n",
    "        input_data: Input data (from input_fn)\n",
    "        model_artifacts: Model artifacts (from model_fn)\n",
    "        \n",
    "    Returns:\n",
    "        dict: Prediction result\n",
    "    \"\"\"\n",
    "    expected_columns = [\n",
    "    'score_3', 'score_4', 'score_5', 'score_6', 'risk_rate', 'last_amount_borrowed',\n",
    "    'last_borrowed_in_months', 'credit_limit', 'income', 'ok_since', 'n_bankruptcies',\n",
    "    'n_defaulted_loans', 'n_accounts', 'n_issues', 'external_data_provider_credit_checks_last_year',\n",
    "    'external_data_provider_email_seen_before', 'reported_income', 'application_time_in_funnel',\n",
    "    'external_data_provider_credit_checks_last_month', 'external_data_provider_fraud_score',\n",
    "    'shipping_state', 'facebook_profile', 'state', 'score_1', 'score_2', 'real_state'\n",
    "    ]\n",
    "    pd.set_option(\"display.max_rows\", None)  # Show all rows\n",
    "    pd.set_option(\"display.max_columns\", None)  # Show all columns\n",
    "    pd.set_option(\"display.width\", 1000)  # Increase column width\n",
    "    pd.set_option(\"display.max_colwidth\", None) \n",
    "\n",
    "    try:\n",
    "        logger.info(\"Starting prediction process\")\n",
    "        \n",
    "        if isinstance(input_data, str):\n",
    "            input_dict = json.loads(input_data)\n",
    "        else:\n",
    "            input_dict = input_data\n",
    "        logger.info(f\"Deserialized input data: {input_dict}\")\n",
    "\n",
    "        # Convert dictionary to DataFrame\n",
    "        user_df = pd.DataFrame([input_dict])\n",
    "        logger.info(f\"Initial DataFrame:\\n{user_df}\")\n",
    "        \n",
    "        # Extract model artifacts\n",
    "        nimputer = model_artifacts['nimputer']\n",
    "        cimputer = model_artifacts['cimputer']\n",
    "        targetencoder = model_artifacts['targetencoder']\n",
    "        ordinalencoder = model_artifacts['ordinalencoder']\n",
    "\n",
    "        before_columns = model_artifacts['before_columns']\n",
    "        xgb_model = model_artifacts['xgb']\n",
    "        # Validate and preprocess the input data\n",
    "        input_data = inference_validator(user_df)\n",
    "        logger.info(f\"input data:\\n{input_data}\")\n",
    "        \n",
    "        logger.info(\"Input data validated\")\n",
    "        \n",
    "        user_df = pd.DataFrame(input_data)\n",
    "        user_df = user_df.reindex(columns=before_columns, fill_value=None)\n",
    "        logger.info(f\"user_df data:\\n{user_df}\")\n",
    "        logger.info(f\"Reindexed user_df columns: {user_df.columns.tolist()}\")\n",
    "        \n",
    "        # Apply imputers\n",
    "        c_features = [f for f in cimputer.feature_names_in_ if f in user_df.columns]\n",
    "        n_features = [f for f in nimputer.feature_names_in_ if f in user_df.columns]\n",
    "        logger.info(f\"c_features: {c_features}\")\n",
    "        logger.info(f\"n_features: {n_features}\")\n",
    "        if c_features:\n",
    "            user_df[c_features] = cimputer.transform(user_df[c_features])\n",
    "            logger.info(\"Applied categorical imputation\")\n",
    "        if n_features:\n",
    "            user_df[n_features] = nimputer.transform(user_df[n_features])\n",
    "            logger.info(\"Applied numerical imputation\")\n",
    "            \n",
    "        logger.info(\"Applied imputation\")\n",
    "        logger.info(f\"user_df data:\\n{user_df}\")\n",
    "        \n",
    "\n",
    "        user_df = feature_engineering(user_df)\n",
    "        logger.info(\"Applied feature engineering\")\n",
    "        user_df = targetencoder.transform(user_df)\n",
    "        user_df = ordinalencoder.transform(user_df)\n",
    "        # Transform using the preprocessor\n",
    "        # # user_processed = inference_preprocessor.transform(user_df)\n",
    "        # logger.info(\"Applied feature preprocessing\")\n",
    "        user_df = user_df.drop(columns=['RiskScore'])\n",
    "        # Make prediction\n",
    "        prediction = xgb_model.predict(user_df)\n",
    "        # prediction_proba = xgb_model.predict_proba(user_df)\n",
    "        logger.info(f\"Generated prediction: {prediction[0]}\")\n",
    "        \n",
    "        result = {\n",
    "            \"prediction\": (prediction[0]).tolist(),\n",
    "            # \"reasoning\": get_risk_score_reasoning(user_df, prediction),\n",
    "        }\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during prediction: {str(e)}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_fn(prediction_output, accept):\n",
    "    \"\"\"\n",
    "    Serialize the prediction output\n",
    "    \n",
    "    Args:\n",
    "        prediction_output: The prediction output from predict_fn\n",
    "        accept: The accept content type\n",
    "        \n",
    "    Returns:\n",
    "        The serialized prediction\n",
    "    \"\"\"\n",
    "    logger.info(f\"Formatting output with accept type: {accept}\")\n",
    "    logger.info(f\"Type of prediction_output: {type(prediction_output)}\")\n",
    "    logger.info(f\"Contents of prediction_output: {prediction_output}\")\n",
    "    \n",
    "    if accept == 'application/json' or accept == '*/*':\n",
    "        try:\n",
    "            # Serialize to JSON\n",
    "            json_output = json.dumps(prediction_output)\n",
    "            logger.info(\"Successfully serialized prediction to JSON\")\n",
    "            return json_output\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error serializing to JSON: {str(e)}\")\n",
    "            raise\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported accept type: {accept}. Only application/json is supported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = {\n",
    "  \"ApplicationDate\": \"2024-03-01\",\n",
    "  \"Age\": 50,\n",
    "  \"CreditScore\": 100,\n",
    "  \"EmploymentStatus\": \"Employed\", #['Employed', 'Self-Employed', 'Unemployed']\n",
    "  \"EducationLevel\": \"Associate\", #['Master', 'Associate', 'Bachelor', 'High School', 'Doctorate']\n",
    "  \"LoanAmount\": 120000,\n",
    "  \"LoanDuration\": 60,\n",
    "  \"MaritalStatus\": \"Single\",#['Married', 'Single', 'Divorced', 'Widowed']\n",
    "  \"NumberOfDependents\": 1,\n",
    "  \"HomeOwnershipStatus\": \"Own\",#['Own', 'Mortgage', 'Rent', 'Other']\n",
    "  \"MonthlyDebtPayments\": 700000,\n",
    "  \"CreditCardUtilizationRate\": 0.6,\n",
    "  \"NumberOfOpenCreditLines\": 2,\n",
    "  \"NumberOfCreditInquiries\": 1,\n",
    "  \"DebtToIncomeRatio\": 0.8,\n",
    "  \"BankruptcyHistory\": 0,\n",
    "  \"LoanPurpose\": \"Home\", #['Home', 'Debt Consolidation', 'Education', 'Other', 'Auto']\n",
    "  \"PreviousLoanDefaults\": 0,\n",
    "  \"PaymentHistory\": 24,\n",
    "  \"LengthOfCreditHistory\": 18,\n",
    "  \"SavingsAccountBalance\": 20000,\n",
    "  \"CheckingAccountBalance\": 15000,\n",
    "  \"TotalAssets\": 2500000,\n",
    "  \"TotalLiabilities\": 200000,\n",
    "  \"MonthlyIncome\": 1100000,\n",
    "  \"UtilityBillsPaymentHistory\": 0.95,\n",
    "  \"JobTenure\": 12,\n",
    "  \"TotalDebtToIncomeRatio\": 0.22\n",
    "}\n",
    "import json\n",
    "artifacts = model_fn(\"saved\")\n",
    "input_data = input_fn(json.dumps(user_input), \"application/json\")\n",
    "result = predict_fn(input_data, artifacts)\n",
    "print(result[\"prediction\"])\n",
    "# print(result[\"reasoning\"])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
