{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import inference_validator, feature_engineering\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', None)  # None means unlimited rows\n",
    "pd.set_option('display.max_columns', None) # None means unlimited columns\n",
    "pd.set_option('display.width', None)      # None means auto-detect width\n",
    "pd.set_option('display.max_colwidth', None) # None means unlimited column width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groq_api_key = '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'\n",
    "import groq\n",
    "\n",
    "# Initialize Groq client\n",
    "client = groq.Client(api_key=\"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\")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def get_risk_score_reasoning(user_df, prediction):\n",
    "    \"\"\"\n",
    "    Generates a reasoning for the risk score using an LLM.\n",
    "\n",
    "    Args:\n",
    "        user_df (pd.DataFrame): User data after feature engineering.\n",
    "        prediction (np.ndarray): The predicted risk score (NumPy array).\n",
    "\n",
    "    Returns:\n",
    "        str: The reasoning for the risk score.\n",
    "    \"\"\"\n",
    "    risk_score = prediction[0]  # Extract the first element (scalar)\n",
    "    user_data_str = user_df.to_string()\n",
    "\n",
    "    # Prepare the prompt for the LLM\n",
    "    prompt = (\n",
    "        f\"You are a financial risk analyst. A user has applied for a loan, and the machine learning model has assigned them a risk score of {risk_score:.2f}.\\n\\n\"\n",
    "        f\"Below are the user's details after feature engineering:\\n\"\n",
    "        f\"{user_data_str}\\n\\n\"\n",
    "        f\"### Instructions:\\n\"\n",
    "        f\"- Provide a detailed explanation of why this person received the given risk score.\\n\"\n",
    "        f\"- Highlight key features that contributed to the risk score (e.g., high debt-to-income ratio, low credit score, late payments, etc.).\\n\"\n",
    "        f\"- Explain how these features impact the user's financial risk.\\n\"\n",
    "        f\"- Mention any factors that could improve or worsen their risk score.\\n\"\n",
    "        f\"- Provide actionable recommendations for the user to reduce their risk score if applicable.\\n\\n\"\n",
    "        f\"- Remember risk score between 27-40 kind of low risk, 40-60 kind of moderate risk and above 60 kind of high risk. more than 75 extremely high. \\n\\n\"\n",
    "        f\"### Output Format:\\n\"\n",
    "        f\"- Output ONLY a plain text explanation without any additional formatting or labels.\\n\"\n",
    "    )\n",
    "\n",
    "    # ... (rest of your LLM interaction code) ...\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"llama-3.1-8b-instant\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a financial risk analyst.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        max_tokens=250,\n",
    "        temperature=0,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import joblib\n",
    "import os\n",
    "import xgboost\n",
    "import json\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    \"\"\"\n",
    "    Load model artifacts from the model_dir for SageMaker model serving\n",
    "    \n",
    "    Args:\n",
    "        model_dir (str): Directory where model artifacts are stored\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing all loaded model artifacts\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Loading model from: {model_dir}\")\n",
    "        \n",
    "        # Load all artifacts from local model_dir\n",
    "        artifacts = {}\n",
    "        \n",
    "        # Load numerical and categorical imputers (using joblib)\n",
    "        artifacts['nimputer'] = joblib.load(os.path.join(model_dir, 'nimputer.joblib'))\n",
    "        logger.info(\"Loaded nimputer\")\n",
    "        \n",
    "        artifacts['cimputer'] = joblib.load(os.path.join(model_dir, 'cimputer.joblib'))\n",
    "        logger.info(\"Loaded cimputer\")\n",
    "        \n",
    "        # Load label encoders (still using pickle)\n",
    "        artifacts['ordinalencoder'] = joblib.load(os.path.join(model_dir, 'ordinalencoder.joblib'))\n",
    "        logger.info(\"Loaded ordinalencoder\")\n",
    "\n",
    "        artifacts['targetencoder'] = joblib.load(os.path.join(model_dir, 'targetencoder.joblib'))\n",
    "        logger.info(\"Loaded targetencoder\")\n",
    "\n",
    "        artifacts['kmean'] = joblib.load(os.path.join(model_dir, 'kmeans.joblib'))\n",
    "\n",
    "        \n",
    "        # Load feature list (JSON remains unchanged)\n",
    "        with open(os.path.join(model_dir, 'before_feature.json'), 'r') as f:\n",
    "            artifacts['before_columns'] = json.load(f)\n",
    "        logger.info(\"Loaded before_columns\")\n",
    "\n",
    "        # Load the model\n",
    "        import catboost\n",
    "\n",
    "        model_path = os.path.join(model_dir, 'cat_model.cbm')\n",
    "        model = catboost.CatBoostRegressor()\n",
    "        model.load_model(model_path)\n",
    "        artifacts['xgb'] = model\n",
    "        logger.info(\"Loaded xgb model\")\n",
    "        \n",
    "        # Add the reasoning dictionary \n",
    "        logger.info(\"All model artifacts loaded successfully\")\n",
    "        return artifacts\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading model: {str(e)}\")\n",
    "        raise\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn(request_body, request_content_type):\n",
    "    \"\"\"\n",
    "    Deserialize and prepare the prediction input\n",
    "    \n",
    "    Args:\n",
    "        request_body: The request body\n",
    "        request_content_type: The request content type\n",
    "        \n",
    "    Returns:\n",
    "        dict: Input data in dictionary format\n",
    "    \"\"\"\n",
    "    logger.info(f\"Received request with content type: {request_content_type}\")\n",
    "    \n",
    "    if request_content_type == 'application/json':\n",
    "        try:\n",
    "            input_data = json.loads(request_body)\n",
    "            logger.info(f\"Parsed input data: {str(input_data)[:100]}...\")\n",
    "            return input_data\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error parsing JSON input: {str(e)}\")\n",
    "            raise ValueError(f\"Error parsing JSON input: {str(e)}\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported content type: {request_content_type}. Only application/json is supported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from encoders import CustomTargetEncoder, CustomOrdinalEncoder\n",
    "\n",
    "def predict_fn(input_data, model_artifacts):\n",
    "    \"\"\"\n",
    "    Apply model to the input data\n",
    "    \n",
    "    Args:\n",
    "        input_data: Input data (from input_fn)\n",
    "        model_artifacts: Model artifacts (from model_fn)\n",
    "        \n",
    "    Returns:\n",
    "        dict: Prediction result\n",
    "    \"\"\"\n",
    "\n",
    "    pd.set_option(\"display.max_rows\", None)  # Show all rows\n",
    "    pd.set_option(\"display.max_columns\", None)  # Show all columns\n",
    "    pd.set_option(\"display.width\", 1000)  # Increase column width\n",
    "    pd.set_option(\"display.max_colwidth\", None) \n",
    "\n",
    "    try:\n",
    "        logger.info(\"Starting prediction process\")\n",
    "        \n",
    "        if isinstance(input_data, str):\n",
    "            input_dict = json.loads(input_data)\n",
    "        else:\n",
    "            input_dict = input_data\n",
    "        logger.info(f\"Deserialized input data: {input_dict}\")\n",
    "\n",
    "        # Convert dictionary to DataFrame\n",
    "        user_df = pd.DataFrame([input_dict])\n",
    "        logger.info(f\"Initial DataFrame:\\n{user_df}\")\n",
    "        \n",
    "        # Extract model artifacts\n",
    "        nimputer = model_artifacts['nimputer']\n",
    "        cimputer = model_artifacts['cimputer']\n",
    "        targetencoder = model_artifacts['targetencoder']\n",
    "        ordinalencoder = model_artifacts['ordinalencoder']\n",
    "        kmean = model_artifacts['kmean']\n",
    "\n",
    "        before_columns = model_artifacts['before_columns']\n",
    "        xgb_model = model_artifacts['xgb']\n",
    "        # Validate and preprocess the input data\n",
    "        input_data = inference_validator(user_df)\n",
    "        logger.info(f\"input data:\\n{input_data}\")\n",
    "        \n",
    "        logger.info(\"Input data validated\")\n",
    "        \n",
    "        user_df = pd.DataFrame(input_data)\n",
    "        user_df = user_df.reindex(columns=before_columns, fill_value=None)\n",
    "        logger.info(f\"user_df data:\\n{user_df}\")\n",
    "        logger.info(f\"Reindexed user_df columns: {user_df.columns.tolist()}\")\n",
    "        \n",
    "        # Apply imputers\n",
    "        c_features = [f for f in cimputer.feature_names_in_ if f in user_df.columns]\n",
    "        n_features = [f for f in nimputer.feature_names_in_ if f in user_df.columns]\n",
    "        logger.info(f\"c_features: {c_features}\")\n",
    "        logger.info(f\"n_features: {n_features}\")\n",
    "        if c_features:\n",
    "            user_df[c_features] = cimputer.transform(user_df[c_features])\n",
    "            logger.info(\"Applied categorical imputation\")\n",
    "        if n_features:\n",
    "            user_df[n_features] = nimputer.transform(user_df[n_features])\n",
    "            logger.info(\"Applied numerical imputation\")\n",
    "            \n",
    "        logger.info(\"Applied imputation\")\n",
    "        logger.info(f\"user_df data:\\n{user_df}\")\n",
    "        \n",
    "        user_df['FinancialCluster'] = kmean.predict(user_df[['CreditCardUtilizationRate', 'MonthlyIncome']])\n",
    "        user_df = feature_engineering(user_df)\n",
    "        logger.info(\"Applied feature engineering\")\n",
    "        user_df = targetencoder.transform(user_df)\n",
    "        user_df = ordinalencoder.transform(user_df)\n",
    "        # Transform using the preprocessor\n",
    "        # # user_processed = inference_preprocessor.transform(user_df)\n",
    "        # logger.info(\"Applied feature preprocessing\")\n",
    "        user_df = user_df.drop(columns=['RiskScore'])\n",
    "        # Make prediction\n",
    "        # print(user_df)\n",
    "        prediction = xgb_model.predict(user_df)\n",
    "        # prediction_proba = xgb_model.predict_proba(user_df)\n",
    "        logger.info(f\"Generated prediction: {prediction[0]}\")\n",
    "        \n",
    "        result = {\n",
    "            \"prediction\": (prediction[0]).tolist(),\n",
    "            # \"reasoning\": get_risk_score_reasoning(user_df, prediction),\n",
    "        }\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during prediction: {str(e)}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_fn(prediction_output, accept):\n",
    "    \"\"\"\n",
    "    Serialize the prediction output\n",
    "    \n",
    "    Args:\n",
    "        prediction_output: The prediction output from predict_fn\n",
    "        accept: The accept content type\n",
    "        \n",
    "    Returns:\n",
    "        The serialized prediction\n",
    "    \"\"\"\n",
    "    logger.info(f\"Formatting output with accept type: {accept}\")\n",
    "    logger.info(f\"Type of prediction_output: {type(prediction_output)}\")\n",
    "    logger.info(f\"Contents of prediction_output: {prediction_output}\")\n",
    "    \n",
    "    if accept == 'application/json' or accept == '*/*':\n",
    "        try:\n",
    "            # Serialize to JSON\n",
    "            json_output = json.dumps(prediction_output)\n",
    "            logger.info(\"Successfully serialized prediction to JSON\")\n",
    "            return json_output\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error serializing to JSON: {str(e)}\")\n",
    "            raise\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported accept type: {accept}. Only application/json is supported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ui = {\n",
    "  \"ApplicationDate\": \"2024-01-01\",\n",
    "  \"Age\": 50,                            # min: 18.00, max: 80.00\n",
    "  \"CreditScore\": 100,                   # min: 343.00, max: 712.00\n",
    "  \"EmploymentStatus\": \"Unemployed\",     # ['Employed', 'Self-Employed', 'Unemployed']\n",
    "  \"EducationLevel\": \"High School\",      # ['Master', 'Associate', 'Bachelor', 'High School', 'Doctorate']\n",
    "  \"LoanAmount\": 120000,                 # min: 3674.00, max: 184732.00\n",
    "  \"LoanDuration\": 60,                   # min: 12.00, max: 120.00\n",
    "  \"MaritalStatus\": \"Widowed\",            # ['Married', 'Single', 'Divorced', 'Widowed']\n",
    "  \"NumberOfDependents\": 1,              # min: 0.00, max: 5.00\n",
    "  \"HomeOwnershipStatus\": \"Own\",         # ['Own', 'Mortgage', 'Rent', 'Other']\n",
    "  \"MonthlyDebtPayments\": 7000,          # min: 50.00, max: 2919.00\n",
    "  \"CreditCardUtilizationRate\": 0.6,     # min: 0.000974, max: 0.917380\n",
    "  \"NumberOfOpenCreditLines\": 2,         # min: 0.00, max: 13.00\n",
    "  \"NumberOfCreditInquiries\": 1,         # min: 0.00, max: 7.00\n",
    "  \"DebtToIncomeRatio\": 0.1,           # min: 0.001720, max: 0.902253\n",
    "  \"BankruptcyHistory\": 0,\n",
    "  \"LoanPurpose\": \"Home\",              # ['Home', 'Debt Consolidation', 'Education', 'Other', 'Auto']\n",
    "  \"PreviousLoanDefaults\": 0,\n",
    "  \"PaymentHistory\": 24,             # min: 8.00, max: 45.00\n",
    "  \"LengthOfCreditHistory\": 18,            # min: 1.00, max: 29.00\n",
    "  \"SavingsAccountBalance\": 20000,         # min: 73.00, max: 200089.00\n",
    "  \"CheckingAccountBalance\": 15000,        # min: 24.00, max: 52572.00\n",
    "  \"TotalAssets\": 250000,                  # min: 2098.00, max: 1,198,472.00\n",
    "  \"TotalLiabilities\": 20000000,           # min: 372.00, max: 1,417,302.00\n",
    "  \"MonthlyIncome\": 11000,                 # min: 1250.00, max: 25000.00\n",
    "  \"UtilityBillsPaymentHistory\": 0.5,      # min: 0.259203, max: 0.999433\n",
    "  \"JobTenure\": 12,                        # min: 0.00, max: 16.00\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = {\n",
    "  \"ApplicationDate\": \"2024-01-21\",\n",
    "  \"Age\": 20,\n",
    "  \"CreditScore\": 550,\n",
    "  \"EmploymentStatus\": \"Unemployed\",\n",
    "  \"EducationLevel\": \"High School\",\n",
    "  \"LoanAmount\": 80000,\n",
    "  \"LoanDuration\": 72,\n",
    "  \"MaritalStatus\": \"Single\",\n",
    "  \"NumberOfDependents\": 3,\n",
    "  \"HomeOwnershipStatus\": \"Rent\",\n",
    "  \"MonthlyDebtPayments\": 2000,\n",
    "  \"CreditCardUtilizationRate\": 0.7,\n",
    "  \"NumberOfOpenCreditLines\": 8,\n",
    "  \"NumberOfCreditInquiries\": 5,\n",
    "  \"BankruptcyHistory\": 1,\n",
    "  \"LoanPurpose\": \"Debt Consolidation\",\n",
    "  \"PreviousLoanDefaults\": 1,\n",
    "  \"PaymentHistory\": 12,\n",
    "  \"LengthOfCreditHistory\": 6,\n",
    "  \"SavingsAccountBalance\": 2000,\n",
    "  \"CheckingAccountBalance\": 1000,\n",
    "  \"TotalAssets\": 50000,\n",
    "  \"TotalLiabilities\": 120000,\n",
    "  \"MonthlyIncome\": 4000,\n",
    "  \"UtilityBillsPaymentHistory\": 0.6,\n",
    "  \"JobTenure\": 2\n",
    "}\n",
    "\n",
    "import json\n",
    "artifacts = model_fn(\"saved\")\n",
    "input_data = input_fn(json.dumps(user_input), \"application/json\")\n",
    "result = predict_fn(input_data, artifacts)\n",
    "print(f'RiskScore = {result[\"prediction\"]}')\n",
    "# print(result[\"reasoning\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
