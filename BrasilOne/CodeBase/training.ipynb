{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV, cross_val_score\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report, roc_curve, roc_auc_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# set the aesthetic style of the plots\n",
    "sns.set_style()\n",
    "\n",
    "# filter warning messages\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# Assuming feature_importance is your DataFrame\n",
    "# ... (your code to create and sort feature_importance) ...\n",
    "\n",
    "# Set options to display all rows and columns\n",
    "pd.set_option('display.max_rows', None)  # None means unlimited rows\n",
    "pd.set_option('display.max_columns', None) # None means unlimited columns\n",
    "pd.set_option('display.width', None)      # None means auto-detect width\n",
    "pd.set_option('display.max_colwidth', None) # None means unlimited column width\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_processed = pd.read_csv('saved/clustered_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "def apply_smote(df, target_column, random_state=42):\n",
    "    # Separate features (X) and target (y)\n",
    "    X = df.drop(target_column, axis=1)\n",
    "    y = df[target_column]\n",
    "\n",
    "    # Apply SMOTE\n",
    "    smote = SMOTE(random_state=random_state)\n",
    "    X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "    # Create a new DataFrame with resampled data\n",
    "    df_resampled = pd.DataFrame(X_resampled, columns=X.columns)\n",
    "    df_resampled[target_column] = y_resampled\n",
    "\n",
    "    return df_resampled\n",
    "\n",
    "X_processed = apply_smote(X_processed, ['Cluster'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "X = X_processed.drop(columns=['Cluster', \"target_default\"])\n",
    "y = X_processed['Cluster']\n",
    "\n",
    "print(\"Cluster in df_normalized:\", 'Cluster' in X_processed.columns)  # Should be True\n",
    "print(\"Cluster in X:\", 'Cluster' in X.columns)  # Should be False\n",
    "\n",
    "\n",
    "\n",
    "f_scores, p_values = f_classif(X, y)\n",
    "\n",
    "# Create a DataFrame to display results\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'F-Score': f_scores,\n",
    "    'P-Value': p_values\n",
    "})\n",
    "\n",
    "# Sort by F-Score (higher F-Score means more important)\n",
    "feature_importance = feature_importance.sort_values(by='F-Score', ascending=False)\n",
    "\n",
    "print(feature_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = X.drop(columns=[\"score_2_bin_x_fraud_score_bin\", \n",
    "#                 \"score_1_bin_x_fraud_score_bin\",\n",
    "#                 \"score_1_bin_x_score_2_bin\",\n",
    "#                 \"score_2_bin\",\n",
    "#                 \"score_1_bin\"])\n",
    "X.nunique().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, stratify=y, test_size=0.1)\n",
    "print(len(X_train), len(X_test))\n",
    "print(\"NaN in X_train:\", X_train.isna().sum().sum())\n",
    "print(\"NaN in y_train:\", y_train.isna().sum())\n",
    "print(\"Infinite values in X:\", np.isinf(y_train.values).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_rus, y_train_rus = (X_train, y_train)\n",
    "\n",
    "f_scores, p_values = f_classif(X_train_rus, y_train_rus)\n",
    "\n",
    "# Create a DataFrame to display feature importance\n",
    "yo = pd.DataFrame({'Feature': X_train_rus.columns, 'F-Score': f_scores, 'P-Value': p_values})\n",
    "\n",
    "# Sort by F-Score (higher F-Score means more important)\n",
    "feature_importance = yo.sort_values(by='F-Score', ascending=False)\n",
    "\n",
    "# Print the feature importance\n",
    "print(feature_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=len(X_train_rus.columns))\n",
    "pca.fit(X_train_rus)\n",
    "\n",
    "# Get explained variance ratio\n",
    "explained_variance = pca.explained_variance_ratio_ * 100\n",
    "\n",
    "# Create DataFrame\n",
    "variance_df = pd.DataFrame({'Feature': X_train_rus.columns, 'Explained Variance (%)': explained_variance})\n",
    "variance_df = variance_df.sort_values(by='Explained Variance (%)', ascending=False)\n",
    "\n",
    "print(variance_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final XGBoost model\n",
    "xgb = XGBClassifier(\n",
    "    max_depth=5, \n",
    "    learning_rate=0.01,  \n",
    "    n_estimators=200, \n",
    "    gamma=1,  \n",
    "    min_child_weight=6,  \n",
    "    # subsample=0.8,  \n",
    "    # colsample_bytree=0.8,  \n",
    "    # reg_lambda=1,  \n",
    "    # reg_alpha=0.1\n",
    ")\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "# xgb = RandomForestClassifier(\n",
    "#     n_estimators=200,  # Number of trees in the forest\n",
    "#     max_depth=8,        # Maximum depth of the trees\n",
    "#     #... other hyperparameters (e.g., min_samples_split, min_samples_leaf, etc.)...\n",
    "#     random_state=42,     # For reproducibility\n",
    "# )\n",
    "\n",
    "# xgb = AdaBoostClassifier(n_estimators=100, learning_rate=0.01, algorithm='SAMME', random_state=42)  # Adjust parameters\n",
    "xgb.fit(X_train_rus, y_train_rus)\n",
    "# prediction\n",
    "X_test_xgb = X_test #scaler.transform(X_test)\n",
    "print(X_test_xgb.head(10))\n",
    "y_pred_xgb = xgb.predict(X_test_xgb)\n",
    "\n",
    "# classification report\n",
    "print(classification_report(y_test, y_pred_xgb))\n",
    "\n",
    "# confusion matrix\n",
    "fig, ax = plt.subplots()\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred_xgb, normalize='true'), annot=True, ax=ax)\n",
    "ax.set_title('Confusion Matrix - XGBoost test')\n",
    "ax.set_xlabel('Predicted Value')\n",
    "ax.set_ylabel('Real Value')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import label_binarize\n",
    "y_pred_xgb = xgb.predict_proba(X_test_xgb)\n",
    "n_classes = len(np.unique(y_test))\n",
    "y_test_binarized = label_binarize(y_test, classes=np.arange(n_classes))\n",
    "\n",
    "auc_score = roc_auc_score(y_test_binarized, y_pred_xgb)\n",
    "print(\"AUC Score:\", auc_score)\n",
    "\n",
    "# Plot ROC curve for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test_binarized[:, i], y_pred_xgb[:, i])\n",
    "    roc_auc[i] = roc_auc_score(y_test_binarized[:, i], y_pred_xgb[:, i])\n",
    "\n",
    "# Plot ROC curves\n",
    "plt.figure(figsize=(8, 6))\n",
    "for i in range(n_classes):\n",
    "    plt.plot(fpr[i], tpr[i], label=f'Class {i} (AUC = {roc_auc[i]:.2f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Random Guess')\n",
    "plt.xlabel('False Positive Rate (FPR)')\n",
    "plt.ylabel('True Positive Rate (TPR)')\n",
    "plt.title('ROC Curve - XGBoost (Multiclass)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save label encoders to disk\n",
    "with open('saved/xgb_model.pkl', 'wb') as f:\n",
    "    pickle.dump(xgb, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
