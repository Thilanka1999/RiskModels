{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV, cross_val_score\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report, roc_curve, roc_auc_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# set the aesthetic style of the plots\n",
    "sns.set_style()\n",
    "\n",
    "# filter warning messages\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# Assuming feature_importance is your DataFrame\n",
    "# ... (your code to create and sort feature_importance) ...\n",
    "\n",
    "# Set options to display all rows and columns\n",
    "pd.set_option('display.max_rows', None)  # None means unlimited rows\n",
    "pd.set_option('display.max_columns', None) # None means unlimited columns\n",
    "pd.set_option('display.width', None)      # None means auto-detect width\n",
    "pd.set_option('display.max_colwidth', None) # None means unlimited column width\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_processed = pd.read_csv('saved/clustered_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "def apply_smote(df, target_column, random_state=42):\n",
    "    # Separate features (X) and target (y)\n",
    "    X = df.drop(target_column, axis=1)\n",
    "    y = df[target_column]\n",
    "\n",
    "    # Apply SMOTE\n",
    "    smote = SMOTE(random_state=random_state)\n",
    "    X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "    # Create a new DataFrame with resampled data\n",
    "    df_resampled = pd.DataFrame(X_resampled, columns=X.columns)\n",
    "    df_resampled[target_column] = y_resampled\n",
    "\n",
    "    return df_resampled\n",
    "\n",
    "X_processed = apply_smote(X_processed, ['Cluster'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_processed.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "# X_processed = X_processed.drop(columns=['AgeBin', 'CreditScoreBin', 'EmploymentStatus', 'MaritalStatus', 'HomeOwnershipStatus', 'EducationLevel', 'LoanPurpose'])\n",
    "\n",
    "X = X_processed.drop(columns=['Cluster', \"LoanApproved\"])\n",
    "y = X_processed['Cluster']\n",
    "print(X.columns)\n",
    "\n",
    "f_scores, p_values = f_classif(X, y)\n",
    "\n",
    "# Create a DataFrame to display results\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'F-Score': f_scores,\n",
    "    'P-Value': p_values\n",
    "})\n",
    "\n",
    "# Sort by F-Score (higher F-Score means more important)\n",
    "feature_importance = feature_importance.sort_values(by='F-Score', ascending=False)\n",
    "\n",
    "print(feature_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, test_size=0.1)\n",
    "print(len(X_train), len(X_test))\n",
    "print(\"NaN in X_train:\", X_train.isna().sum().sum())\n",
    "print(\"NaN in y_train:\", y_train.isna().sum())\n",
    "print(\"Infinite values in X:\", np.isinf(y_train.values).sum())\n",
    "\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "with open('saved/df_train_encoded.json', 'w') as f:\n",
    "    json.dump(X_train.columns.tolist(), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_rus, y_train_rus = (X_train, y_train)\n",
    "\n",
    "f_scores, p_values = f_classif(X_train_rus, y_train_rus)\n",
    "\n",
    "# Create a DataFrame to display feature importance\n",
    "yo = pd.DataFrame({'Feature': X_train_rus.columns, 'F-Score': f_scores, 'P-Value': p_values})\n",
    "\n",
    "# Sort by F-Score (higher F-Score means more important)\n",
    "feature_importance = yo.sort_values(by='F-Score', ascending=False)\n",
    "\n",
    "# Print the feature importance\n",
    "print(feature_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=len(X_train_rus.columns))\n",
    "pca.fit(X_train_rus)\n",
    "\n",
    "# Get explained variance ratio\n",
    "explained_variance = pca.explained_variance_ratio_ * 100\n",
    "\n",
    "# Create DataFrame\n",
    "variance_df = pd.DataFrame({'Feature': X_train_rus.columns, 'Explained Variance (%)': explained_variance})\n",
    "variance_df = variance_df.sort_values(by='Explained Variance (%)', ascending=False)\n",
    "\n",
    "# print(variance_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final XGBoost model\n",
    "xgb = XGBClassifier(\n",
    "    max_depth=9, \n",
    "    learning_rate=0.1,  \n",
    "    n_estimators=200, \n",
    "    gamma=1,  \n",
    "    min_child_weight=3,\n",
    "    random_state=42  # Ensures reproducibility\n",
    ")\n",
    "\n",
    "xgb.fit(X_train_rus, y_train_rus)\n",
    "# prediction\n",
    "X_test_xgb = X_test #scaler.transform(X_test)\n",
    "y_pred_xgb = xgb.predict(X_test_xgb)\n",
    "\n",
    "\n",
    "# classification report\n",
    "# print(classification_report(y_test, y_pred_xgb))\n",
    "\n",
    "# confusion matrix\n",
    "fig, ax = plt.subplots()\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred_xgb, normalize='true'), annot=True, ax=ax)\n",
    "ax.set_title('Confusion Matrix - XGBoost test')\n",
    "ax.set_xlabel('Predicted Value')\n",
    "ax.set_ylabel('Real Value')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# from sklearn.preprocessing import label_binarize\n",
    "# y_pred_xgb = xgb.predict_proba(X_test_xgb)\n",
    "# n_classes = len(np.unique(y_test))\n",
    "# y_test_binarized = label_binarize(y_test, classes=np.arange(n_classes))\n",
    "\n",
    "# auc_score = roc_auc_score(y_test_binarized, y_pred_xgb)\n",
    "# print(\"AUC Score:\", auc_score)\n",
    "\n",
    "# # Plot ROC curve for each class\n",
    "# fpr = dict()\n",
    "# tpr = dict()\n",
    "# roc_auc = dict()\n",
    "# for i in range(n_classes):\n",
    "#     fpr[i], tpr[i], _ = roc_curve(y_test_binarized[:, i], y_pred_xgb[:, i])\n",
    "#     roc_auc[i] = roc_auc_score(y_test_binarized[:, i], y_pred_xgb[:, i])\n",
    "\n",
    "# # Plot ROC curves\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# for i in range(n_classes):\n",
    "#     plt.plot(fpr[i], tpr[i], label=f'Class {i} (AUC = {roc_auc[i]:.2f})')\n",
    "\n",
    "# plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Random Guess')\n",
    "# plt.xlabel('False Positive Rate (FPR)')\n",
    "# plt.ylabel('True Positive Rate (TPR)')\n",
    "# plt.title('ROC Curve - XGBoost (Multiclass)')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save label encoders to disk\n",
    "with open('saved/xgb_model.pkl', 'wb') as f:\n",
    "    pickle.dump(xgb, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
