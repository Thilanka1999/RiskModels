{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import Model\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SageMaker Role ARN: arn:aws:iam::796932308591:role/service-role/SageMaker-ExecutionRole-20250214T145019\n"
     ]
    }
   ],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "print(f\"SageMaker Role ARN: {role}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import inference_validator, reasoning, feature_engineering, additional_feature_engineering\n",
    "import pickle, json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import boto3\n",
    "# import pickle\n",
    "# import json\n",
    "# import io\n",
    "# # Initialize S3 client\n",
    "# s3 = boto3.client('s3')\n",
    "\n",
    "# # Define S3 bucket and file path\n",
    "# bucket = 'mitrailabs-personaclassification'\n",
    "\n",
    "# # Function to load pickle files from S3\n",
    "# def load_pickle_from_s3(bucket, key):\n",
    "#     response = s3.get_object(Bucket=bucket, Key=key)\n",
    "#     return pickle.load(io.BytesIO(response['Body'].read()))\n",
    "\n",
    "# # Function to load JSON file from S3\n",
    "# def load_json_from_s3(bucket, key):\n",
    "#     response = s3.get_object(Bucket=bucket, Key=key)\n",
    "#     # response_json\n",
    "#     # io.BytesIO(file_obj['Body'].read())\n",
    "#     return json.load(response['Body']) \n",
    "\n",
    "# # Load all objects from S3\n",
    "# model_prefix = 'risk_prediction/Intermediate_states/'\n",
    "\n",
    "# nimputer = load_pickle_from_s3(bucket, f\"{model_prefix}nimputer.pkl\")\n",
    "# cimputer = load_pickle_from_s3(bucket, f\"{model_prefix}cimputer.pkl\")\n",
    "# label_encoders = load_pickle_from_s3(bucket, f\"{model_prefix}label_encoders.pkl\")\n",
    "# inference_preprocessor = load_pickle_from_s3(bucket, f\"{model_prefix}inference_preprocessor.pkl\")\n",
    "\n",
    "# before_columns = load_json_from_s3(bucket, f\"{model_prefix}before_feature.json\")\n",
    "\n",
    "\n",
    "# model_prefix = 'risk_prediction/models/'\n",
    "\n",
    "# xgb = load_pickle_from_s3(bucket, f\"{model_prefix}xgb_model.pkl\")\n",
    "\n",
    "# # ✅ Now all objects are loaded from S3 into memory\n",
    "# print(\"All files loaded successfully from S3!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3 = boto3.client('s3')\n",
    "\n",
    "# bucket = 'mitrailabs-personaclassification'\n",
    "\n",
    "\n",
    "# model_prefix = 'risk_prediction/Intermediate_states/'\n",
    "# nimputer_response = s3.get_object(\n",
    "#     Bucket=bucket,\n",
    "#     Key=f\"{model_prefix}nimputer.pkl\"\n",
    "# )\n",
    "\n",
    "# nimputer = pickle.load(nimputer_response['Body'])\n",
    "\n",
    "\n",
    "# with open('saved/nimputer.pkl', 'rb') as f:\n",
    "#     nimputer = pickle.load(f)\n",
    "\n",
    "# with open('saved/cimputer.pkl', 'rb') as f:\n",
    "#     cimputer = pickle.load(f)\n",
    "\n",
    "# with open('saved/label_encoders.pkl', 'rb') as f:\n",
    "#     label_encoders = pickle.load(f)\n",
    "\n",
    "    \n",
    "# with open('saved/inference_preprocessor.pkl', 'rb') as f:\n",
    "#     inference_preprocessor = pickle.load(f)\n",
    "\n",
    "# with open('saved/xgb_model.pkl', 'rb') as f:\n",
    "#     xgb = pickle.load(f)\n",
    "\n",
    "# with open('saved/before_feature.json', 'r') as f:\n",
    "#     before_columns = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files loaded successfully from S3!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator SimpleImputer from version 1.5.1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import joblib\n",
    "import json\n",
    "import io\n",
    "import xgboost\n",
    "\n",
    "# Initialize S3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Define S3 bucket\n",
    "bucket = 'mitrailabs-personaclassification'\n",
    "\n",
    "# Function to load joblib files from S3\n",
    "def load_joblib_from_s3(bucket, key):\n",
    "    response = s3.get_object(Bucket=bucket, Key=key)\n",
    "    return joblib.load(io.BytesIO(response['Body'].read()))\n",
    "\n",
    "# Function to load JSON file from S3 (unchanged)\n",
    "def load_json_from_s3(bucket, key):\n",
    "    response = s3.get_object(Bucket=bucket, Key=key)\n",
    "    return json.load(response['Body'])\n",
    "\n",
    "def load_xgboost_from_s3(bucket, key):\n",
    "    response = s3.get_object(Bucket=bucket, Key=key)\n",
    "    model_data = response['Body'].read()\n",
    "    \n",
    "    # Save to a temporary file\n",
    "    temp_model_path = 'saved/xgb_model.json'\n",
    "    with open(temp_model_path, 'wb') as f:\n",
    "        f.write(model_data)\n",
    "    \n",
    "    # Load the model\n",
    "    model = xgboost.Booster()\n",
    "    model.load_model(temp_model_path)\n",
    "    return model\n",
    "\n",
    "# Load all objects from S3\n",
    "model_prefix = 'risk_prediction/Intermediate_states/'\n",
    "nimputer = load_joblib_from_s3(bucket, f\"{model_prefix}nimputer.joblib\")\n",
    "cimputer = load_joblib_from_s3(bucket, f\"{model_prefix}cimputer.joblib\")\n",
    "label_encoders = load_joblib_from_s3(bucket, f\"{model_prefix}label_encoders.joblib\")\n",
    "# inference_preprocessor = load_joblib_from_s3(bucket, f\"{model_prefix}inference_preprocessor.joblib\")\n",
    "before_columns = load_json_from_s3(bucket, f\"{model_prefix}before_feature.json\")\n",
    "\n",
    "model_prefix = 'risk_prediction/models/'\n",
    "xgb = load_xgboost_from_s3(bucket, f\"{model_prefix}xgb_model.json\")\n",
    "\n",
    "# ✅ Now all objects are loaded from S3 into memory\n",
    "print(\"All files loaded successfully from S3!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0\n",
      "Name: fraud_score_bin, dtype: int64\n",
      "Prediction: [[0.03845232 0.03844284 0.03844432 0.03844078 0.8462198 ]]\n",
      "Low-Medium Risk \n",
      " This cluster has a low default rate (0.119) and shows good overall reliability. Borrowers in this cluster have taken moderate-sized loans (last_amount_borrowed: 0.064) and have a low debt-to-income ratio (0.051). Their credit utilization (0.007) is minimal, and their risk scores (score_1, score_2) are moderate. Their behavior suggests slightly more caution is needed compared to low-risk borrowers, but they are generally reliable.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Example user input\n",
    "user_input = {\n",
    "    'score_3': 0.5, 'score_4': np.nan, 'score_5': 0.7, 'score_6': np.nan,\n",
    "    'risk_rate': 0.2, 'last_amount_borrowed': 1000, 'last_borrowed_in_months': 12,\n",
    "    'credit_limit': 5000, 'income': np.nan, 'ok_since': 24, 'n_bankruptcies': 9,\n",
    "    'n_defaulted_loans': 1, 'n_accounts': 3, 'n_issues': 2,\n",
    "    'external_data_provider_credit_checks_last_year': 3,\n",
    "    'external_data_provider_email_seen_before': 1, 'reported_income': 50000,\n",
    "    'application_time_in_funnel': 5, 'external_data_provider_credit_checks_last_month': 0,\n",
    "    'external_data_provider_fraud_score': 50, 'shipping_state': np.nan,\n",
    "    'facebook_profile': True, 'state': np.nan, 'score_1': np.nan, 'score_2': np.nan, \"real_state\":'a'\n",
    "}\n",
    "\n",
    "# user_input = {\n",
    "#     'score_3': 510, 'score_4': np.nan, 'score_5': 0.480, 'score_6': 92,\n",
    "#     'risk_rate': 0.45, 'last_amount_borrowed': 600, 'last_borrowed_in_months': 20,\n",
    "#     'credit_limit': 500, 'income': 60000, 'ok_since': 32, 'n_bankruptcies': 3.0,\n",
    "#     'n_defaulted_loans': 0, 'n_accounts': 25.0, 'n_issues': 5.0,\n",
    "#     'external_data_provider_credit_checks_last_year': 4.0,\n",
    "#     'external_data_provider_email_seen_before': 5, 'reported_income': 50000,\n",
    "#     'application_time_in_funnel': 900, 'external_data_provider_credit_checks_last_month': 5.0,\n",
    "#     'external_data_provider_fraud_score': 250, 'shipping_state': np.nan,\n",
    "#     'facebook_profile': True, 'state': np.nan, 'score_1': '1Rk8w4Ucd5yR3KcqZzLdow==', 'score_2': np.nan, \"real_state\": np.nan\n",
    "# }\n",
    "\n",
    "user_input = {\n",
    "    'score_3': 800,  # High credit score\n",
    "    'score_4': np.nan,  # Missing value\n",
    "    'score_5': 0.850,  # High probability score\n",
    "    'score_6': 98,  # High score\n",
    "    'risk_rate': 0.10,  # Low risk rate\n",
    "    'last_amount_borrowed': 200,  # Low amount borrowed\n",
    "    'last_borrowed_in_months': 36,  # Long time since last borrowing\n",
    "    'credit_limit': 10,  # High credit limit\n",
    "    'income': 120000,  # High income\n",
    "    'ok_since': 60,  # Long time since last issue\n",
    "    'n_bankruptcies': 0.0,  # No bankruptcies\n",
    "    'n_defaulted_loans': 0,  # No defaulted loans\n",
    "    'n_accounts': 10.0,  # Moderate number of accounts\n",
    "    'n_issues': 1.0,  # Few issues\n",
    "    'external_data_provider_credit_checks_last_year': 1.0,  # Few credit checks\n",
    "    'external_data_provider_email_seen_before': 10,  # Email seen many times (trusted)\n",
    "    'reported_income': 120000,  # High reported income\n",
    "    'application_time_in_funnel': 300,  # Short application time\n",
    "    'external_data_provider_credit_checks_last_month': 0.0,  # No recent credit checks\n",
    "    'external_data_provider_fraud_score': 100,  # Low fraud score\n",
    "    'shipping_state': np.nan,  # Missing value\n",
    "    'facebook_profile': False,  # Has a Facebook profile\n",
    "    'state': np.nan,  # Missing value\n",
    "    'score_1': '1Rk8w4Ucd5yR3KcqZzLdow==',  # Example encrypted score\n",
    "    'score_2': np.nan,  # Missing value\n",
    "    'real_state': np.nan  # Missing value\n",
    "}\n",
    "\n",
    "user_input = inference_validator(user_input)\n",
    "user_df = pd.DataFrame([user_input])\n",
    "# Ensure the user input has the same columns as the training data\n",
    "user_df = user_df.reindex(columns=before_columns, fill_value=np.nan)\n",
    "\n",
    "# Assuming you know which features each imputer was trained on\n",
    "c_features = [f for f in cimputer.feature_names_in_ if f in user_df.columns]\n",
    "n_features = [f for f in nimputer.feature_names_in_ if f in user_df.columns]\n",
    "# print(c_features)\n",
    "# print(n_features)\n",
    "\n",
    "# Apply only if features exist\n",
    "if c_features:\n",
    "    user_df[c_features] = cimputer.transform(user_df[c_features])\n",
    "if n_features:\n",
    "    user_df[n_features] = nimputer.transform(user_df[n_features])\n",
    "\n",
    "user_df = user_df.drop(columns=[\"target_default\"])\n",
    "# print(\"target_default\" in user_df.columns)\n",
    "# Apply the same label encoding to the user input\n",
    "for col in c_features:\n",
    "    if col in user_df.columns and col != \"target_default\":\n",
    "        # Use the transform method of the fitted LabelEncoder\n",
    "        user_df[col] = label_encoders[col].transform(user_df[col])\n",
    "\n",
    "yo_features = [i for i in c_features + n_features if i != \"target_default\"]\n",
    "# user_df = user_df.reindex(columns=yo_features)\n",
    "# user_df = user_df.drop(columns=[\"target_default\"])\n",
    "\n",
    "user_df = feature_engineering(user_df)\n",
    "user_df = additional_feature_engineering(user_df)\n",
    "# print(user_df)\n",
    "# user_processed = inference_preprocessor.transform(user_df)\n",
    "print(user_df['fraud_score_bin'])\n",
    "# user_df['fraud_score_bin'] = user_df['fraud_score_bin'].astype(int)\n",
    "user_df = xgboost.DMatrix(user_df)\n",
    "# Make prediction\n",
    "prediction = xgb.predict(user_df)\n",
    "# prediction_proba = xgb.predict_proba(user_df)\n",
    "\n",
    "print(\"Prediction:\", prediction)\n",
    "print(reasoning[str(np.argmax((prediction[0])))])\n",
    "# print(\"Prediction Probability:\", prediction_proba)\n",
    "# print(user_processed)\n",
    "# 6 → 3 → 7 → 1 → 2 → 0 → 5 → 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
