{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8083cd49-d188-4c60-9346-601184f9bd4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import Model\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d91b9afd-b530-4383-837b-5e1b183b98fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SageMaker Role ARN: arn:aws:iam::796932308591:role/service-role/SageMaker-ExecutionRole-20250214T145019\n"
     ]
    }
   ],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "print(f\"SageMaker Role ARN: {role}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85f89aaf-0730-4c65-9d7b-2a22e8ee179e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import boto3\n",
    "# import os\n",
    "# import tarfile\n",
    "# import shutil\n",
    "\n",
    "# # Initialize S3 client\n",
    "# s3 = boto3.client('s3')\n",
    "\n",
    "# # Define S3 bucket and file paths\n",
    "# bucket = 'mitrailabs-personaclassification'\n",
    "# model_prefix = 'risk_prediction/Intermediate_states/'\n",
    "# model_files = [\n",
    "#     'nimputer.joblib',  \n",
    "#     'cimputer.joblib',  \n",
    "#     'label_encoders.joblib',  # Keeping as pickle  \n",
    "#     'inference_preprocessor.joblib',  \n",
    "#     'before_feature.json'  \n",
    "# ]\n",
    "\n",
    "# model_files_s3_keys = [f\"{model_prefix}{file}\" for file in model_files]\n",
    "\n",
    "# # Add the XGBoost model (now using .joblib)\n",
    "# xgb_model_s3_key = 'risk_prediction/models/xgb_model.joblib'\n",
    "# model_files_s3_keys.append(xgb_model_s3_key)\n",
    "\n",
    "# # Define the local artifacts directory\n",
    "# artifacts_dir = 'artifacts'\n",
    "# os.makedirs(artifacts_dir, exist_ok=True)  # Create the directory if it doesn't exist\n",
    "\n",
    "# # Download all files from S3\n",
    "# for s3_key in model_files_s3_keys:\n",
    "#     file_name = os.path.basename(s3_key)\n",
    "#     local_path = os.path.join(artifacts_dir, file_name)\n",
    "#     s3.download_file(bucket, s3_key, local_path)\n",
    "#     print(f\"Downloaded {s3_key} to {local_path}\")\n",
    "\n",
    "# # Copy the inference script to the artifacts directory\n",
    "# inference_script_path = os.path.join(artifacts_dir, 'inference.py')\n",
    "# if os.path.exists('inference.py'):\n",
    "#     shutil.copy('inference.py', inference_script_path)\n",
    "#     print(f\"Copied inference.py to {artifacts_dir}\")\n",
    "# else:\n",
    "#     print(\"inference.py does not exist in the current directory.\")\n",
    "#     # Handle missing inference.py as needed (e.g., raise an error or create a placeholder)\n",
    "\n",
    "# # Copy the requirements.txt file to the artifacts directory\n",
    "# requirements_file_path = 'requirements.txt'  # Path to your requirements.txt\n",
    "# if os.path.exists(requirements_file_path):\n",
    "#     shutil.copy(requirements_file_path, artifacts_dir)\n",
    "#     print(f\"Copied {requirements_file_path} to {artifacts_dir}\")\n",
    "# else:\n",
    "#     print(f\"{requirements_file_path} does not exist in the current directory.\")\n",
    "#     # Handle missing requirements.txt as needed (e.g., create an empty file, raise an error)\n",
    "\n",
    "# # Create a .tar.gz file from the artifacts directory\n",
    "# tar_gz_path = os.path.join(artifacts_dir, 'model.tar.gz')\n",
    "# with tarfile.open(tar_gz_path, 'w:gz') as tar:\n",
    "#     for file_name in os.listdir(artifacts_dir):\n",
    "#         file_path = os.path.join(artifacts_dir, file_name)\n",
    "#         tar.add(file_path, arcname=file_name)\n",
    "# print(f\"Created {tar_gz_path}\")\n",
    "\n",
    "# # Upload the .tar.gz file to S3\n",
    "# s3_key = 'risk_prediction/models/model.tar.gz'\n",
    "# s3.upload_file(tar_gz_path, bucket, s3_key)\n",
    "# print(f\"Uploaded {tar_gz_path} to s3://{bucket}/{s3_key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3a82ddc3-2483-4ea0-be4c-584c5216df9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded risk_prediction/Intermediate_states/nimputer.joblib to artifacts/nimputer.joblib\n",
      "Downloaded risk_prediction/Intermediate_states/cimputer.joblib to artifacts/cimputer.joblib\n",
      "Downloaded risk_prediction/Intermediate_states/label_encoders.joblib to artifacts/label_encoders.joblib\n",
      "Downloaded risk_prediction/Intermediate_states/before_feature.json to artifacts/before_feature.json\n",
      "Downloaded risk_prediction/models/xgb_model.json to artifacts/xgb_model.json\n",
      "Copied inference.py to artifacts/code\n",
      "Copied requirements.txt to artifacts/code\n",
      "Created artifacts/model.tar.gz\n",
      "Uploaded artifacts/model.tar.gz to s3://mitrailabs-personaclassification/risk_prediction/models/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import os\n",
    "import tarfile\n",
    "import shutil\n",
    "\n",
    "# Initialize S3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Define S3 bucket and file paths\n",
    "bucket = 'mitrailabs-personaclassification'\n",
    "model_prefix = 'risk_prediction/Intermediate_states/'\n",
    "model_files = [\n",
    "    'nimputer.joblib',  \n",
    "    'cimputer.joblib',  \n",
    "    'label_encoders.joblib',  # Keeping as pickle  \n",
    "    # 'inference_preprocessor.joblib',  \n",
    "    'before_feature.json'  \n",
    "]\n",
    "\n",
    "model_files_s3_keys = [f\"{model_prefix}{file}\" for file in model_files]\n",
    "\n",
    "# Add the XGBoost model (now using .joblib)\n",
    "xgb_model_s3_key = 'risk_prediction/models/xgb_model.json'\n",
    "model_files_s3_keys.append(xgb_model_s3_key)\n",
    "\n",
    "# Define the local artifacts directory\n",
    "artifacts_dir = 'artifacts'\n",
    "os.makedirs(artifacts_dir, exist_ok=True)  # Create the directory if it doesn't exist\n",
    "\n",
    "# Download all files from S3\n",
    "for s3_key in model_files_s3_keys:\n",
    "    file_name = os.path.basename(s3_key)\n",
    "    local_path = os.path.join(artifacts_dir, file_name)\n",
    "    s3.download_file(bucket, s3_key, local_path)\n",
    "    print(f\"Downloaded {s3_key} to {local_path}\")\n",
    "\n",
    "# Create a 'code' directory inside the artifacts directory\n",
    "code_dir = os.path.join(artifacts_dir, 'code')\n",
    "os.makedirs(code_dir, exist_ok=True)\n",
    "\n",
    "# Copy the inference script to the 'code' directory\n",
    "inference_script_path = os.path.join(code_dir, 'inference.py')\n",
    "if os.path.exists('inference.py'):\n",
    "    shutil.copy('inference.py', inference_script_path)\n",
    "    print(f\"Copied inference.py to {code_dir}\")\n",
    "else:\n",
    "    print(\"inference.py does not exist in the current directory.\")\n",
    "    # Handle missing inference.py as needed (e.g., raise an error or create a placeholder)\n",
    "\n",
    "# Copy the requirements.txt file to the 'code' directory\n",
    "requirements_file_path = 'requirements.txt'  # Path to your requirements.txt\n",
    "if os.path.exists(requirements_file_path):\n",
    "    shutil.copy(requirements_file_path, code_dir)\n",
    "    print(f\"Copied {requirements_file_path} to {code_dir}\")\n",
    "else:\n",
    "    print(f\"{requirements_file_path} does not exist in the current directory.\")\n",
    "    # Handle missing requirements.txt as needed (e.g., create an empty file, raise an error)\n",
    "\n",
    "# Create a .tar.gz file from the artifacts directory\n",
    "tar_gz_path = os.path.join(artifacts_dir, 'model.tar.gz')\n",
    "with tarfile.open(tar_gz_path, 'w:gz') as tar:\n",
    "    # Add model files directly to the root of the tar\n",
    "    for file_name in os.listdir(artifacts_dir):\n",
    "        if file_name != 'code' and file_name != 'model.tar.gz':\n",
    "            file_path = os.path.join(artifacts_dir, file_name)\n",
    "            tar.add(file_path, arcname=file_name)\n",
    "    \n",
    "    # Add the 'code' directory to the tar\n",
    "    for file_name in os.listdir(code_dir):\n",
    "        file_path = os.path.join(code_dir, file_name)\n",
    "        tar.add(file_path, arcname=os.path.join('code', file_name))\n",
    "\n",
    "print(f\"Created {tar_gz_path}\")\n",
    "\n",
    "# Upload the .tar.gz file to S3\n",
    "s3_key = 'risk_prediction/models/model.tar.gz'\n",
    "s3.upload_file(tar_gz_path, bucket, s3_key)\n",
    "print(f\"Uploaded {tar_gz_path} to s3://{bucket}/{s3_key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59641a9f-fef9-4751-89c1-9ec6a8edd545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.xgboost.model import XGBoostModel\n",
    "from sagemaker.xgboost import XGBoost\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.model import Model\n",
    "# import sklearn\n",
    "# import xgboost\n",
    "\n",
    "# with open(\"requirements.txt\", \"w\") as f:\n",
    "#     f.write(f\"scikit-learn=={sklearn.__version__}\\n\")\n",
    "#     f.write(f\"xgboost=={xgboost.__version__}\\n\") # Add xgboost\n",
    "#     f.write(\"pandas\\n\") # Add other dependencies\n",
    "#     f.write(\"numpy\\n\")\n",
    "\n",
    "target_bucket = 'mitrailabs-personaclassification'\n",
    "model_path = 'risk_prediction/models/model.tar.gz'\n",
    "model_s3_path = f's3://{target_bucket}/{model_path}'\n",
    "\n",
    "model = XGBoostModel(\n",
    "    model_data=model_s3_path,\n",
    "    role=role,\n",
    "    entry_point='inference.py',\n",
    "    framework_version='1.7-1',  # Matching closer to your 2.1.4\n",
    "    py_version='py3',\n",
    "    # dependencies=['requirements2.txt'],\n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "# model = XGBoost(\n",
    "#     model_data=model_s3_path,\n",
    "#     role=role,\n",
    "#     entry_point='inference.py',\n",
    "#     framework_version='1.5-1',  # Matching closer to your 2.1.4\n",
    "#     py_version='py3',\n",
    "#     sagemaker_session=sagemaker_session,\n",
    "#     requirements_file = 'requirements.txt'\n",
    "# )\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9555ed2-fd74-4a67-8b9a-f09bb98c9951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------!"
     ]
    }
   ],
   "source": [
    "predictor = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.t2.medium',  # Cost-effective choice\n",
    "    endpoint_name='customer-creditrisk-V1',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70384896-ef1d-471d-a80b-51dd6be3e7f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction Response:\n",
      "{'prediction': 3, 'reasoning': 'Medium Risk \\n Borrowers in this cluster have a moderate default rate (0.117) and show mixed borrowing behavior. They have taken very small loans (last_amount_borrowed: 0.008) and have a low debt-to-income ratio (0.007). Their credit utilization (0.001) is minimal, but their risk scores (score_1, score_2) show more variation, suggesting the need for closer monitoring. Their behavior is generally positive but requires caution.', 'prediction_proba': [0.038472529500722885, 0.038463044911623, 0.03846452385187149, 0.8460456728935242, 0.038554225116968155]}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "user_input = {\n",
    "    'score_3': 800,  # High credit score\n",
    "    'score_4': np.nan,  # Missing value\n",
    "    'score_5': 0.850,  # High probability score\n",
    "    'score_6': 98,  # High score\n",
    "    'risk_rate': 0.10,  # Low risk rate\n",
    "    'last_amount_borrowed': 200,  # Low amount borrowed\n",
    "    'last_borrowed_in_months': 36,  # Long time since last borrowing\n",
    "    'credit_limit': 10,  # High credit limit\n",
    "    'income': np.nan,  # High income\n",
    "    'ok_since': 60,  # Long time since last issue\n",
    "    'n_bankruptcies': 0.0,  # No bankruptcies\n",
    "    'n_defaulted_loans': 0,  # No defaulted loans\n",
    "    'n_accounts': 10.0,  # Moderate number of accounts\n",
    "    'n_issues': 1.0,  # Few issues\n",
    "    'external_data_provider_credit_checks_last_year': 1.0,  # Few credit checks\n",
    "    'external_data_provider_email_seen_before': 10,  # Email seen many times (trusted)\n",
    "    'reported_income': 120000,  # High reported income\n",
    "    'application_time_in_funnel': 300,  # Short application time\n",
    "    'external_data_provider_credit_checks_last_month': 0.0,  # No recent credit checks\n",
    "    'external_data_provider_fraud_score': 100,  # Low fraud score\n",
    "    'shipping_state': np.nan,  # Missing value\n",
    "    'facebook_profile': True,  # Has a Facebook profile\n",
    "    'state': np.nan,  # Missing value\n",
    "    'score_1': '1Rk8w4Ucd5yR3KcqZzLdow==',  # Example encrypted score\n",
    "    'score_2': np.nan,  # Missing value\n",
    "    'real_state': np.nan  # Missing value\n",
    "}\n",
    "\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "endpoint_name = \"customer-creditrisk-V1\"\n",
    "\n",
    "# Create a predictor object\n",
    "predictor = Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer()\n",
    ")\n",
    "\n",
    "\n",
    "input_data = json.dumps(user_input)\n",
    "response = predictor.predict(input_data)\n",
    "print(\"\\nPrediction Response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "34269b73-9ddf-40bb-acae-60ebf81a8085",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"prediction\": 3, \"reasoning\": \"fnejfn fjenfijn\", \"prediction_proba\": [1, 2, 3, 4]}'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "prediction = [[1, 2, 3, 4]]\n",
    "reasoning = {'1':1, '2':2, '3':\"fnejfn fjenfijn\", '4':4}\n",
    "result = {\n",
    "            \"prediction\": np.argmax((prediction[0])).tolist(),\n",
    "            \"reasoning\": reasoning[str(np.argmax((prediction[0])))],\n",
    "            \"prediction_proba\": prediction[0]\n",
    "        }\n",
    "\n",
    "json.dumps(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f337c52-2521-4dad-9db5-34d59c4401a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listing endpoints...\n",
      "Found 1 endpoints to delete.\n",
      "\n",
      "Deleting endpoint: customer-creditrisk-V1\n",
      "Waiting for endpoint customer-creditrisk-V1 to be deleted...\n",
      "Endpoint customer-creditrisk-V1 deleted successfully.\n",
      "Endpoint configuration customer-creditrisk-V1 deleted.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "def delete_all_endpoints(name_filter=None):\n",
    "    sm_client = boto3.client('sagemaker')\n",
    "\n",
    "    print(\"Listing endpoints...\")\n",
    "    endpoints = sm_client.list_endpoints()['Endpoints']\n",
    "\n",
    "    if name_filter:\n",
    "        endpoints = [ep for ep in endpoints if name_filter in ep['EndpointName']]\n",
    "\n",
    "    if not endpoints:\n",
    "        print(\"No endpoints found to delete.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(endpoints)} endpoints to delete.\")\n",
    "\n",
    "    for endpoint in endpoints:\n",
    "        endpoint_name = endpoint['EndpointName']\n",
    "        print(f\"\\nDeleting endpoint: {endpoint_name}\")\n",
    "\n",
    "        try:\n",
    "            sm_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "            print(f\"Waiting for endpoint {endpoint_name} to be deleted...\")\n",
    "\n",
    "            # Use a waiter with exponential backoff\n",
    "            waiter = sm_client.get_waiter('endpoint_deleted')\n",
    "            waiter.wait(EndpointName=endpoint_name, \n",
    "                        WaiterConfig={'Delay': 5, 'MaxAttempts': 30})\n",
    "\n",
    "            print(f\"Endpoint {endpoint_name} deleted successfully.\")\n",
    "\n",
    "        except sm_client.exceptions.ResourceNotFound:  # Catch ResourceNotFound\n",
    "            print(f\"Endpoint {endpoint_name} not found. It may have already been deleted.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error deleting endpoint {endpoint_name}: {str(e)}\")\n",
    "\n",
    "        # Delete endpoint config (optional)\n",
    "        try:\n",
    "            sm_client.delete_endpoint_config(EndpointConfigName=endpoint_name)\n",
    "            print(f\"Endpoint configuration {endpoint_name} deleted.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not delete endpoint config {endpoint_name}: {str(e)}\")\n",
    "\n",
    "delete_all_endpoints()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44174586-fedb-45f2-98f2-07b70f6cea01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
